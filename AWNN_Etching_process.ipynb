{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "scaler3 = MinMaxScaler()\n",
    "scaler4 = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.   170.     7.91]\n",
      " [  1.   170.     8.01]\n",
      " [  1.   170.     8.06]\n",
      " [  0.8  170.     7.57]\n",
      " [  0.8  170.     7.65]\n",
      " [  0.8  170.     7.65]\n",
      " [  0.8  170.     7.78]\n",
      " [  0.5  170.     6.79]\n",
      " [  0.5  170.     6.91]\n",
      " [  0.5  170.     6.98]\n",
      " [  0.3  170.     5.87]\n",
      " [  0.3  170.     6.02]\n",
      " [  0.3  170.     6.22]\n",
      " [  0.2  170.     5.09]\n",
      " [  0.2  170.     5.3 ]\n",
      " [  1.    68.     4.07]\n",
      " [  1.    68.     3.91]\n",
      " [  1.    68.     4.08]\n",
      " [  0.9   68.     3.98]\n",
      " [  0.8   68.     3.86]\n",
      " [  0.8   68.     3.9 ]\n",
      " [  0.8   68.     3.91]\n",
      " [  0.5   68.     3.44]\n",
      " [  0.5   68.     3.46]\n",
      " [  0.5   68.     3.54]\n",
      " [  0.3   68.     3.02]\n",
      " [  0.3   68.     3.07]\n",
      " [  0.3   68.     3.14]\n",
      " [  0.2   68.     2.48]\n",
      " [  0.2   68.     2.66]\n",
      " [  0.2   68.     2.78]\n",
      " [  1.    51.     3.15]\n",
      " [  1.    51.     3.17]\n",
      " [  1.    51.     3.23]\n",
      " [  0.9   51.     3.12]\n",
      " [  0.9   51.     3.12]\n",
      " [  0.9   51.     3.18]\n",
      " [  0.8   51.     3.06]\n",
      " [  0.8   51.     3.06]\n",
      " [  0.8   51.     3.06]\n",
      " [  0.8   51.     3.1 ]\n",
      " [  0.5   51.     2.74]\n",
      " [  0.5   51.     2.74]\n",
      " [  0.5   51.     2.8 ]\n",
      " [  0.3   51.     2.38]\n",
      " [  0.3   51.     2.41]\n",
      " [  0.3   51.     2.5 ]\n",
      " [  1.    34.     2.2 ]\n",
      " [  1.    34.     2.2 ]\n",
      " [  1.    34.     2.25]\n",
      " [  0.8   34.     2.13]\n",
      " [  0.8   34.     2.09]\n",
      " [  0.8   34.     2.11]\n",
      " [  0.8   34.     2.15]\n",
      " [  0.5   34.     1.92]\n",
      " [  0.5   34.     1.95]\n",
      " [  0.5   34.     1.96]\n",
      " [  0.3   34.     1.66]\n",
      " [  0.3   34.     1.68]\n",
      " [  0.2   34.     1.46]]\n"
     ]
    }
   ],
   "source": [
    "hidden_nodes=50\n",
    "df = pandas.read_excel('exp.xlsx', sheet_name='exp')\n",
    "par=30\n",
    "temp=df.values\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.91 8.01 8.06 7.57 7.65 7.65 7.78 6.79 6.91 6.98 5.87 6.02 6.22 5.09\n",
      " 5.3  4.07 3.91 4.08 3.98 3.86 3.9  3.91 3.44 3.46 3.54 3.02 3.07 3.14\n",
      " 2.48 2.66 2.78 3.15 3.17 3.23 3.12 3.12 3.18 3.06 3.06 3.06 3.1  2.74\n",
      " 2.74 2.8  2.38 2.41 2.5  2.2  2.2  2.25 2.13 2.09 2.11 2.15 1.92 1.95\n",
      " 1.96 1.66 1.68 1.46]\n"
     ]
    }
   ],
   "source": [
    "aaa=temp[:,2]\n",
    "\n",
    "print(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.91]\n",
      " [8.01]\n",
      " [8.06]\n",
      " [7.57]\n",
      " [7.65]\n",
      " [7.65]\n",
      " [7.78]\n",
      " [6.79]\n",
      " [6.91]\n",
      " [6.98]\n",
      " [5.87]\n",
      " [6.02]\n",
      " [6.22]\n",
      " [5.09]\n",
      " [5.3 ]\n",
      " [4.07]\n",
      " [3.91]\n",
      " [4.08]\n",
      " [3.98]\n",
      " [3.86]\n",
      " [3.9 ]\n",
      " [3.91]\n",
      " [3.44]\n",
      " [3.46]\n",
      " [3.54]\n",
      " [3.02]\n",
      " [3.07]\n",
      " [3.14]\n",
      " [2.48]\n",
      " [2.66]\n",
      " [2.78]\n",
      " [3.15]\n",
      " [3.17]\n",
      " [3.23]\n",
      " [3.12]\n",
      " [3.12]\n",
      " [3.18]\n",
      " [3.06]\n",
      " [3.06]\n",
      " [3.06]\n",
      " [3.1 ]\n",
      " [2.74]\n",
      " [2.74]\n",
      " [2.8 ]\n",
      " [2.38]\n",
      " [2.41]\n",
      " [2.5 ]\n",
      " [2.2 ]\n",
      " [2.2 ]\n",
      " [2.25]\n",
      " [2.13]\n",
      " [2.09]\n",
      " [2.11]\n",
      " [2.15]\n",
      " [1.92]\n",
      " [1.95]\n",
      " [1.96]\n",
      " [1.66]\n",
      " [1.68]\n",
      " [1.46]]\n"
     ]
    }
   ],
   "source": [
    "aaa=aaa.reshape((-1,1))\n",
    "\n",
    "print(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp:= [[1.         1.         0.97727273]\n",
      " [1.         1.         0.99242424]\n",
      " [1.         1.         1.        ]\n",
      " [0.75       1.         0.92575758]\n",
      " [0.75       1.         0.93787879]\n",
      " [0.75       1.         0.93787879]\n",
      " [0.75       1.         0.95757576]\n",
      " [0.375      1.         0.80757576]\n",
      " [0.375      1.         0.82575758]\n",
      " [0.375      1.         0.83636364]\n",
      " [0.125      1.         0.66818182]\n",
      " [0.125      1.         0.69090909]\n",
      " [0.125      1.         0.72121212]\n",
      " [0.         1.         0.55      ]\n",
      " [0.         1.         0.58181818]\n",
      " [1.         0.25       0.39545455]\n",
      " [1.         0.25       0.37121212]\n",
      " [1.         0.25       0.3969697 ]\n",
      " [0.875      0.25       0.38181818]\n",
      " [0.75       0.25       0.36363636]\n",
      " [0.75       0.25       0.36969697]\n",
      " [0.75       0.25       0.37121212]\n",
      " [0.375      0.25       0.3       ]\n",
      " [0.375      0.25       0.3030303 ]\n",
      " [0.375      0.25       0.31515152]\n",
      " [0.125      0.25       0.23636364]\n",
      " [0.125      0.25       0.24393939]\n",
      " [0.125      0.25       0.25454545]\n",
      " [0.         0.25       0.15454545]\n",
      " [0.         0.25       0.18181818]\n",
      " [0.         0.25       0.2       ]\n",
      " [1.         0.125      0.25606061]\n",
      " [1.         0.125      0.25909091]\n",
      " [1.         0.125      0.26818182]\n",
      " [0.875      0.125      0.25151515]\n",
      " [0.875      0.125      0.25151515]\n",
      " [0.875      0.125      0.26060606]\n",
      " [0.75       0.125      0.24242424]\n",
      " [0.75       0.125      0.24242424]\n",
      " [0.75       0.125      0.24242424]\n",
      " [0.75       0.125      0.24848485]\n",
      " [0.375      0.125      0.19393939]\n",
      " [0.375      0.125      0.19393939]\n",
      " [0.375      0.125      0.2030303 ]\n",
      " [0.125      0.125      0.13939394]\n",
      " [0.125      0.125      0.14393939]\n",
      " [0.125      0.125      0.15757576]\n",
      " [1.         0.         0.11212121]\n",
      " [1.         0.         0.11212121]\n",
      " [1.         0.         0.11969697]\n",
      " [0.75       0.         0.10151515]\n",
      " [0.75       0.         0.09545455]\n",
      " [0.75       0.         0.09848485]\n",
      " [0.75       0.         0.10454545]\n",
      " [0.375      0.         0.06969697]\n",
      " [0.375      0.         0.07424242]\n",
      " [0.375      0.         0.07575758]\n",
      " [0.125      0.         0.03030303]\n",
      " [0.125      0.         0.03333333]\n",
      " [0.         0.         0.        ]]\n",
      "X:= [[1.    1.   ]\n",
      " [1.    1.   ]\n",
      " [1.    1.   ]\n",
      " [0.75  1.   ]\n",
      " [0.75  1.   ]\n",
      " [0.75  1.   ]\n",
      " [0.75  1.   ]\n",
      " [0.375 1.   ]\n",
      " [0.375 1.   ]\n",
      " [0.375 1.   ]\n",
      " [0.125 1.   ]\n",
      " [0.125 1.   ]\n",
      " [0.125 1.   ]\n",
      " [0.    1.   ]\n",
      " [0.    1.   ]\n",
      " [1.    0.25 ]\n",
      " [1.    0.25 ]\n",
      " [1.    0.25 ]\n",
      " [0.875 0.25 ]\n",
      " [0.75  0.25 ]\n",
      " [0.75  0.25 ]\n",
      " [0.75  0.25 ]\n",
      " [0.375 0.25 ]\n",
      " [0.375 0.25 ]\n",
      " [0.375 0.25 ]\n",
      " [0.125 0.25 ]\n",
      " [0.125 0.25 ]\n",
      " [0.125 0.25 ]\n",
      " [0.    0.25 ]\n",
      " [0.    0.25 ]\n",
      " [0.    0.25 ]\n",
      " [1.    0.125]\n",
      " [1.    0.125]\n",
      " [1.    0.125]\n",
      " [0.875 0.125]\n",
      " [0.875 0.125]\n",
      " [0.875 0.125]\n",
      " [0.75  0.125]\n",
      " [0.75  0.125]\n",
      " [0.75  0.125]\n",
      " [0.75  0.125]\n",
      " [0.375 0.125]\n",
      " [0.375 0.125]\n",
      " [0.375 0.125]\n",
      " [0.125 0.125]\n",
      " [0.125 0.125]\n",
      " [0.125 0.125]\n",
      " [1.    0.   ]\n",
      " [1.    0.   ]\n",
      " [1.    0.   ]\n",
      " [0.75  0.   ]\n",
      " [0.75  0.   ]\n",
      " [0.75  0.   ]\n",
      " [0.75  0.   ]\n",
      " [0.375 0.   ]\n",
      " [0.375 0.   ]\n",
      " [0.375 0.   ]\n",
      " [0.125 0.   ]\n",
      " [0.125 0.   ]\n",
      " [0.    0.   ]]\n",
      "y:= [0.97727273 0.99242424 1.         0.92575758 0.93787879 0.93787879\n",
      " 0.95757576 0.80757576 0.82575758 0.83636364 0.66818182 0.69090909\n",
      " 0.72121212 0.55       0.58181818 0.39545455 0.37121212 0.3969697\n",
      " 0.38181818 0.36363636 0.36969697 0.37121212 0.3        0.3030303\n",
      " 0.31515152 0.23636364 0.24393939 0.25454545 0.15454545 0.18181818\n",
      " 0.2        0.25606061 0.25909091 0.26818182 0.25151515 0.25151515\n",
      " 0.26060606 0.24242424 0.24242424 0.24242424 0.24848485 0.19393939\n",
      " 0.19393939 0.2030303  0.13939394 0.14393939 0.15757576 0.11212121\n",
      " 0.11212121 0.11969697 0.10151515 0.09545455 0.09848485 0.10454545\n",
      " 0.06969697 0.07424242 0.07575758 0.03030303 0.03333333 0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "scaler2.fit(aaa)\n",
    "\n",
    "scaler1.fit(temp)\n",
    "\n",
    "temp=scaler1.transform(temp)\n",
    "\n",
    "print(\"temp:=\", temp)\n",
    "\n",
    "X= temp[:,range(0,2)]\n",
    "\n",
    "print(\"X:=\", X)\n",
    "\n",
    "y= temp[:,2]\n",
    "\n",
    "print(\"y:=\", y)\n",
    "\n",
    "# Splitting of data\n",
    "\n",
    "X_train, X_test = X[:par], X[par:]\n",
    "y_train, y_test = y[:par], y[par:]\n",
    "\n",
    "df = pandas.read_excel('sim.xlsx', sheet_name='sim')\n",
    "temp=df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa=temp[:,2]\n",
    "aaa=aaa.reshape((-1,1))\n",
    "scaler4.fit(aaa)\n",
    "temp=scaler3.fit_transform(temp)\n",
    "temp=temp[:,2]\n",
    "y_sim=temp[:par]\n",
    "ytest_sim=temp[par:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.08011127\n",
      "Iteration 2, loss = 0.07873691\n",
      "Iteration 3, loss = 0.07737900\n",
      "Iteration 4, loss = 0.07603715\n",
      "Iteration 5, loss = 0.07471163\n",
      "Iteration 6, loss = 0.07340323\n",
      "Iteration 7, loss = 0.07211138\n",
      "Iteration 8, loss = 0.07083573\n",
      "Iteration 9, loss = 0.06957731\n",
      "Iteration 10, loss = 0.06833635\n",
      "Iteration 11, loss = 0.06711306\n",
      "Iteration 12, loss = 0.06590763\n",
      "Iteration 13, loss = 0.06472361\n",
      "Iteration 14, loss = 0.06357137\n",
      "Iteration 15, loss = 0.06243767\n",
      "Iteration 16, loss = 0.06132218\n",
      "Iteration 17, loss = 0.06022790\n",
      "Iteration 18, loss = 0.05917052\n",
      "Iteration 19, loss = 0.05813202\n",
      "Iteration 20, loss = 0.05711228\n",
      "Iteration 21, loss = 0.05611122\n",
      "Iteration 22, loss = 0.05512863\n",
      "Iteration 23, loss = 0.05416425\n",
      "Iteration 24, loss = 0.05321857\n",
      "Iteration 25, loss = 0.05229089\n",
      "Iteration 26, loss = 0.05138065\n",
      "Iteration 27, loss = 0.05048948\n",
      "Iteration 28, loss = 0.04962412\n",
      "Iteration 29, loss = 0.04879170\n",
      "Iteration 30, loss = 0.04797615\n",
      "Iteration 31, loss = 0.04717724\n",
      "Iteration 32, loss = 0.04639621\n",
      "Iteration 33, loss = 0.04563298\n",
      "Iteration 34, loss = 0.04488570\n",
      "Iteration 35, loss = 0.04415410\n",
      "Iteration 36, loss = 0.04343792\n",
      "Iteration 37, loss = 0.04273690\n",
      "Iteration 38, loss = 0.04205420\n",
      "Iteration 39, loss = 0.04138929\n",
      "Iteration 40, loss = 0.04073895\n",
      "Iteration 41, loss = 0.04010290\n",
      "Iteration 42, loss = 0.03948078\n",
      "Iteration 43, loss = 0.03887318\n",
      "Iteration 44, loss = 0.03827899\n",
      "Iteration 45, loss = 0.03769795\n",
      "Iteration 46, loss = 0.03712772\n",
      "Iteration 47, loss = 0.03657013\n",
      "Iteration 48, loss = 0.03602490\n",
      "Iteration 49, loss = 0.03549175\n",
      "Iteration 50, loss = 0.03497049\n",
      "Iteration 51, loss = 0.03446089\n",
      "Iteration 52, loss = 0.03396274\n",
      "Iteration 53, loss = 0.03347707\n",
      "Iteration 54, loss = 0.03300367\n",
      "Iteration 55, loss = 0.03254126\n",
      "Iteration 56, loss = 0.03208956\n",
      "Iteration 57, loss = 0.03164833\n",
      "Iteration 58, loss = 0.03121700\n",
      "Iteration 59, loss = 0.03079579\n",
      "Iteration 60, loss = 0.03038423\n",
      "Iteration 61, loss = 0.02998129\n",
      "Iteration 62, loss = 0.02958750\n",
      "Iteration 63, loss = 0.02920218\n",
      "Iteration 64, loss = 0.02882566\n",
      "Iteration 65, loss = 0.02845754\n",
      "Iteration 66, loss = 0.02809670\n",
      "Iteration 67, loss = 0.02774410\n",
      "Iteration 68, loss = 0.02739953\n",
      "Iteration 69, loss = 0.02706282\n",
      "Iteration 70, loss = 0.02674106\n",
      "Iteration 71, loss = 0.02642804\n",
      "Iteration 72, loss = 0.02612175\n",
      "Iteration 73, loss = 0.02582300\n",
      "Iteration 74, loss = 0.02553185\n",
      "Iteration 75, loss = 0.02524729\n",
      "Iteration 76, loss = 0.02496801\n",
      "Iteration 77, loss = 0.02469427\n",
      "Iteration 78, loss = 0.02442642\n",
      "Iteration 79, loss = 0.02416580\n",
      "Iteration 80, loss = 0.02391270\n",
      "Iteration 81, loss = 0.02366509\n",
      "Iteration 82, loss = 0.02342271\n",
      "Iteration 83, loss = 0.02318538\n",
      "Iteration 84, loss = 0.02295308\n",
      "Iteration 85, loss = 0.02272931\n",
      "Iteration 86, loss = 0.02251536\n",
      "Iteration 87, loss = 0.02230393\n",
      "Iteration 88, loss = 0.02209653\n",
      "Iteration 89, loss = 0.02189359\n",
      "Iteration 90, loss = 0.02169409\n",
      "Iteration 91, loss = 0.02149813\n",
      "Iteration 92, loss = 0.02130571\n",
      "Iteration 93, loss = 0.02111724\n",
      "Iteration 94, loss = 0.02093312\n",
      "Iteration 95, loss = 0.02075250\n",
      "Iteration 96, loss = 0.02057872\n",
      "Iteration 97, loss = 0.02041039\n",
      "Iteration 98, loss = 0.02024432\n",
      "Iteration 99, loss = 0.02008055\n",
      "Iteration 100, loss = 0.01991907\n",
      "Iteration 101, loss = 0.01975988\n",
      "Iteration 102, loss = 0.01960295\n",
      "Iteration 103, loss = 0.01945284\n",
      "Iteration 104, loss = 0.01930488\n",
      "Iteration 105, loss = 0.01915865\n",
      "Iteration 106, loss = 0.01901448\n",
      "Iteration 107, loss = 0.01887241\n",
      "Iteration 108, loss = 0.01873242\n",
      "Iteration 109, loss = 0.01859498\n",
      "Iteration 110, loss = 0.01846008\n",
      "Iteration 111, loss = 0.01832715\n",
      "Iteration 112, loss = 0.01819715\n",
      "Iteration 113, loss = 0.01807459\n",
      "Iteration 114, loss = 0.01795638\n",
      "Iteration 115, loss = 0.01784095\n",
      "Iteration 116, loss = 0.01772651\n",
      "Iteration 117, loss = 0.01761306\n",
      "Iteration 118, loss = 0.01750194\n",
      "Iteration 119, loss = 0.01739347\n",
      "Iteration 120, loss = 0.01728631\n",
      "Iteration 121, loss = 0.01718042\n",
      "Iteration 122, loss = 0.01707577\n",
      "Iteration 123, loss = 0.01697232\n",
      "Iteration 124, loss = 0.01687004\n",
      "Iteration 125, loss = 0.01676887\n",
      "Iteration 126, loss = 0.01666955\n",
      "Iteration 127, loss = 0.01657097\n",
      "Iteration 128, loss = 0.01647261\n",
      "Iteration 129, loss = 0.01637635\n",
      "Iteration 130, loss = 0.01628098\n",
      "Iteration 131, loss = 0.01618642\n",
      "Iteration 132, loss = 0.01609263\n",
      "Iteration 133, loss = 0.01599960\n",
      "Iteration 134, loss = 0.01590731\n",
      "Iteration 135, loss = 0.01581574\n",
      "Iteration 136, loss = 0.01572487\n",
      "Iteration 137, loss = 0.01563488\n",
      "Iteration 138, loss = 0.01554565\n",
      "Iteration 139, loss = 0.01545712\n",
      "Iteration 140, loss = 0.01536953\n",
      "Iteration 141, loss = 0.01528235\n",
      "Iteration 142, loss = 0.01519579\n",
      "Iteration 143, loss = 0.01511023\n",
      "Iteration 144, loss = 0.01502766\n",
      "Iteration 145, loss = 0.01494563\n",
      "Iteration 146, loss = 0.01486413\n",
      "Iteration 147, loss = 0.01478316\n",
      "Iteration 148, loss = 0.01470316\n",
      "Iteration 149, loss = 0.01462277\n",
      "Iteration 150, loss = 0.01454109\n",
      "Iteration 151, loss = 0.01445953\n",
      "Iteration 152, loss = 0.01437799\n",
      "Iteration 153, loss = 0.01429666\n",
      "Iteration 154, loss = 0.01421554\n",
      "Iteration 155, loss = 0.01413468\n",
      "Iteration 156, loss = 0.01405407\n",
      "Iteration 157, loss = 0.01397373\n",
      "Iteration 158, loss = 0.01389368\n",
      "Iteration 159, loss = 0.01381391\n",
      "Iteration 160, loss = 0.01373444\n",
      "Iteration 161, loss = 0.01365527\n",
      "Iteration 162, loss = 0.01357640\n",
      "Iteration 163, loss = 0.01349784\n",
      "Iteration 164, loss = 0.01341959\n",
      "Iteration 165, loss = 0.01334165\n",
      "Iteration 166, loss = 0.01326401\n",
      "Iteration 167, loss = 0.01318669\n",
      "Iteration 168, loss = 0.01310968\n",
      "Iteration 169, loss = 0.01303297\n",
      "Iteration 170, loss = 0.01295657\n",
      "Iteration 171, loss = 0.01288049\n",
      "Iteration 172, loss = 0.01280446\n",
      "Iteration 173, loss = 0.01272854\n",
      "Iteration 174, loss = 0.01265279\n",
      "Iteration 175, loss = 0.01257723\n",
      "Iteration 176, loss = 0.01250188\n",
      "Iteration 177, loss = 0.01242674\n",
      "Iteration 178, loss = 0.01235182\n",
      "Iteration 179, loss = 0.01227713\n",
      "Iteration 180, loss = 0.01220268\n",
      "Iteration 181, loss = 0.01212846\n",
      "Iteration 182, loss = 0.01205450\n",
      "Iteration 183, loss = 0.01198078\n",
      "Iteration 184, loss = 0.01190732\n",
      "Iteration 185, loss = 0.01183412\n",
      "Iteration 186, loss = 0.01176175\n",
      "Iteration 187, loss = 0.01169115\n",
      "Iteration 188, loss = 0.01162066\n",
      "Iteration 189, loss = 0.01155030\n",
      "Iteration 190, loss = 0.01148009\n",
      "Iteration 191, loss = 0.01141006\n",
      "Iteration 192, loss = 0.01134022\n",
      "Iteration 193, loss = 0.01127058\n",
      "Iteration 194, loss = 0.01120116\n",
      "Iteration 195, loss = 0.01113198\n",
      "Iteration 196, loss = 0.01106304\n",
      "Iteration 197, loss = 0.01099434\n",
      "Iteration 198, loss = 0.01092590\n",
      "Iteration 199, loss = 0.01085772\n",
      "Iteration 200, loss = 0.01078981\n",
      "Iteration 201, loss = 0.01072218\n",
      "Iteration 202, loss = 0.01065481\n",
      "Iteration 203, loss = 0.01058693\n",
      "Iteration 204, loss = 0.01051817\n",
      "Iteration 205, loss = 0.01044945\n",
      "Iteration 206, loss = 0.01038081\n",
      "Iteration 207, loss = 0.01031310\n",
      "Iteration 208, loss = 0.01024536\n",
      "Iteration 209, loss = 0.01017758\n",
      "Iteration 210, loss = 0.01010981\n",
      "Iteration 211, loss = 0.01004209\n",
      "Iteration 212, loss = 0.00997445\n",
      "Iteration 213, loss = 0.00990871\n",
      "Iteration 214, loss = 0.00984681\n",
      "Iteration 215, loss = 0.00978467\n",
      "Iteration 216, loss = 0.00972223\n",
      "Iteration 217, loss = 0.00965955\n",
      "Iteration 218, loss = 0.00959670\n",
      "Iteration 219, loss = 0.00953375\n",
      "Iteration 220, loss = 0.00947074\n",
      "Iteration 221, loss = 0.00940929\n",
      "Iteration 222, loss = 0.00934834\n",
      "Iteration 223, loss = 0.00928740\n",
      "Iteration 224, loss = 0.00922651\n",
      "Iteration 225, loss = 0.00916583\n",
      "Iteration 226, loss = 0.00910793\n",
      "Iteration 227, loss = 0.00905066\n",
      "Iteration 228, loss = 0.00899302\n",
      "Iteration 229, loss = 0.00893506\n",
      "Iteration 230, loss = 0.00887724\n",
      "Iteration 231, loss = 0.00881947\n",
      "Iteration 232, loss = 0.00876167\n",
      "Iteration 233, loss = 0.00870388\n",
      "Iteration 234, loss = 0.00864612\n",
      "Iteration 235, loss = 0.00858994\n",
      "Iteration 236, loss = 0.00853445\n",
      "Iteration 237, loss = 0.00847888\n",
      "Iteration 238, loss = 0.00842327\n",
      "Iteration 239, loss = 0.00836766\n",
      "Iteration 240, loss = 0.00831208\n",
      "Iteration 241, loss = 0.00825657\n",
      "Iteration 242, loss = 0.00820125\n",
      "Iteration 243, loss = 0.00814716\n",
      "Iteration 244, loss = 0.00809344\n",
      "Iteration 245, loss = 0.00803978\n",
      "Iteration 246, loss = 0.00798607\n",
      "Iteration 247, loss = 0.00793232\n",
      "Iteration 248, loss = 0.00787859\n",
      "Iteration 249, loss = 0.00782595\n",
      "Iteration 250, loss = 0.00777381\n",
      "Iteration 251, loss = 0.00772159\n",
      "Iteration 252, loss = 0.00766945\n",
      "Iteration 253, loss = 0.00761736\n",
      "Iteration 254, loss = 0.00756516\n",
      "Iteration 255, loss = 0.00751319\n",
      "Iteration 256, loss = 0.00746194\n",
      "Iteration 257, loss = 0.00741064\n",
      "Iteration 258, loss = 0.00735927\n",
      "Iteration 259, loss = 0.00730851\n",
      "Iteration 260, loss = 0.00725805\n",
      "Iteration 261, loss = 0.00720762\n",
      "Iteration 262, loss = 0.00715725\n",
      "Iteration 263, loss = 0.00710756\n",
      "Iteration 264, loss = 0.00705841\n",
      "Iteration 265, loss = 0.00700925\n",
      "Iteration 266, loss = 0.00696036\n",
      "Iteration 267, loss = 0.00691181\n",
      "Iteration 268, loss = 0.00686326\n",
      "Iteration 269, loss = 0.00681506\n",
      "Iteration 270, loss = 0.00676706\n",
      "Iteration 271, loss = 0.00671912\n",
      "Iteration 272, loss = 0.00667150\n",
      "Iteration 273, loss = 0.00662404\n",
      "Iteration 274, loss = 0.00657690\n",
      "Iteration 275, loss = 0.00652973\n",
      "Iteration 276, loss = 0.00648210\n",
      "Iteration 277, loss = 0.00643576\n",
      "Iteration 278, loss = 0.00638967\n",
      "Iteration 279, loss = 0.00634365\n",
      "Iteration 280, loss = 0.00629761\n",
      "Iteration 281, loss = 0.00625162\n",
      "Iteration 282, loss = 0.00620572\n",
      "Iteration 283, loss = 0.00615989\n",
      "Iteration 284, loss = 0.00611415\n",
      "Iteration 285, loss = 0.00606880\n",
      "Iteration 286, loss = 0.00602351\n",
      "Iteration 287, loss = 0.00597848\n",
      "Iteration 288, loss = 0.00593361\n",
      "Iteration 289, loss = 0.00588901\n",
      "Iteration 290, loss = 0.00584386\n",
      "Iteration 291, loss = 0.00579789\n",
      "Iteration 292, loss = 0.00575196\n",
      "Iteration 293, loss = 0.00570609\n",
      "Iteration 294, loss = 0.00566031\n",
      "Iteration 295, loss = 0.00561528\n",
      "Iteration 296, loss = 0.00557029\n",
      "Iteration 297, loss = 0.00552537\n",
      "Iteration 298, loss = 0.00548056\n",
      "Iteration 299, loss = 0.00543604\n",
      "Iteration 300, loss = 0.00539168\n",
      "Iteration 301, loss = 0.00534750\n",
      "Iteration 302, loss = 0.00530341\n",
      "Iteration 303, loss = 0.00525955\n",
      "Iteration 304, loss = 0.00521609\n",
      "Iteration 305, loss = 0.00517305\n",
      "Iteration 306, loss = 0.00513034\n",
      "Iteration 307, loss = 0.00508789\n",
      "Iteration 308, loss = 0.00504565\n",
      "Iteration 309, loss = 0.00500364\n",
      "Iteration 310, loss = 0.00496187\n",
      "Iteration 311, loss = 0.00492038\n",
      "Iteration 312, loss = 0.00487918\n",
      "Iteration 313, loss = 0.00483841\n",
      "Iteration 314, loss = 0.00479785\n",
      "Iteration 315, loss = 0.00475750\n",
      "Iteration 316, loss = 0.00471738\n",
      "Iteration 317, loss = 0.00467771\n",
      "Iteration 318, loss = 0.00463855\n",
      "Iteration 319, loss = 0.00459951\n",
      "Iteration 320, loss = 0.00456068\n",
      "Iteration 321, loss = 0.00452208\n",
      "Iteration 322, loss = 0.00448368\n",
      "Iteration 323, loss = 0.00444571\n",
      "Iteration 324, loss = 0.00440796\n",
      "Iteration 325, loss = 0.00437001\n",
      "Iteration 326, loss = 0.00433204\n",
      "Iteration 327, loss = 0.00429443\n",
      "Iteration 328, loss = 0.00425716\n",
      "Iteration 329, loss = 0.00422010\n",
      "Iteration 330, loss = 0.00418330\n",
      "Iteration 331, loss = 0.00414677\n",
      "Iteration 332, loss = 0.00411051\n",
      "Iteration 333, loss = 0.00407478\n",
      "Iteration 334, loss = 0.00403943\n",
      "Iteration 335, loss = 0.00400425\n",
      "Iteration 336, loss = 0.00396937\n",
      "Iteration 337, loss = 0.00393475\n",
      "Iteration 338, loss = 0.00390078\n",
      "Iteration 339, loss = 0.00386765\n",
      "Iteration 340, loss = 0.00383473\n",
      "Iteration 341, loss = 0.00380205\n",
      "Iteration 342, loss = 0.00376965\n",
      "Iteration 343, loss = 0.00373740\n",
      "Iteration 344, loss = 0.00370544\n",
      "Iteration 345, loss = 0.00367370\n",
      "Iteration 346, loss = 0.00364222\n",
      "Iteration 347, loss = 0.00361098\n",
      "Iteration 348, loss = 0.00357997\n",
      "Iteration 349, loss = 0.00354921\n",
      "Iteration 350, loss = 0.00351888\n",
      "Iteration 351, loss = 0.00348867\n",
      "Iteration 352, loss = 0.00345867\n",
      "Iteration 353, loss = 0.00342903\n",
      "Iteration 354, loss = 0.00339958\n",
      "Iteration 355, loss = 0.00337037\n",
      "Iteration 356, loss = 0.00334144\n",
      "Iteration 357, loss = 0.00331276\n",
      "Iteration 358, loss = 0.00328430\n",
      "Iteration 359, loss = 0.00325608\n",
      "Iteration 360, loss = 0.00322806\n",
      "Iteration 361, loss = 0.00320033\n",
      "Iteration 362, loss = 0.00317278\n",
      "Iteration 363, loss = 0.00314555\n",
      "Iteration 364, loss = 0.00311851\n",
      "Iteration 365, loss = 0.00309171\n",
      "Iteration 366, loss = 0.00306516\n",
      "Iteration 367, loss = 0.00303883\n",
      "Iteration 368, loss = 0.00301272\n",
      "Iteration 369, loss = 0.00298683\n",
      "Iteration 370, loss = 0.00296120\n",
      "Iteration 371, loss = 0.00293571\n",
      "Iteration 372, loss = 0.00291049\n",
      "Iteration 373, loss = 0.00288545\n",
      "Iteration 374, loss = 0.00286064\n",
      "Iteration 375, loss = 0.00283604\n",
      "Iteration 376, loss = 0.00281165\n",
      "Iteration 377, loss = 0.00278748\n",
      "Iteration 378, loss = 0.00276355\n",
      "Iteration 379, loss = 0.00273982\n",
      "Iteration 380, loss = 0.00271624\n",
      "Iteration 381, loss = 0.00269291\n",
      "Iteration 382, loss = 0.00266979\n",
      "Iteration 383, loss = 0.00264686\n",
      "Iteration 384, loss = 0.00262414\n",
      "Iteration 385, loss = 0.00260162\n",
      "Iteration 386, loss = 0.00257928\n",
      "Iteration 387, loss = 0.00255713\n",
      "Iteration 388, loss = 0.00253520\n",
      "Iteration 389, loss = 0.00251348\n",
      "Iteration 390, loss = 0.00249195\n",
      "Iteration 391, loss = 0.00247060\n",
      "Iteration 392, loss = 0.00244943\n",
      "Iteration 393, loss = 0.00242846\n",
      "Iteration 394, loss = 0.00240768\n",
      "Iteration 395, loss = 0.00238706\n",
      "Iteration 396, loss = 0.00236664\n",
      "Iteration 397, loss = 0.00234615\n",
      "Iteration 398, loss = 0.00232581\n",
      "Iteration 399, loss = 0.00230565\n",
      "Iteration 400, loss = 0.00228562\n",
      "Iteration 401, loss = 0.00226577\n",
      "Iteration 402, loss = 0.00224610\n",
      "Iteration 403, loss = 0.00222658\n",
      "Iteration 404, loss = 0.00220723\n",
      "Iteration 405, loss = 0.00218808\n",
      "Iteration 406, loss = 0.00216909\n",
      "Iteration 407, loss = 0.00215027\n",
      "Iteration 408, loss = 0.00213162\n",
      "Iteration 409, loss = 0.00211314\n",
      "Iteration 410, loss = 0.00209485\n",
      "Iteration 411, loss = 0.00207671\n",
      "Iteration 412, loss = 0.00205876\n",
      "Iteration 413, loss = 0.00204098\n",
      "Iteration 414, loss = 0.00202338\n",
      "Iteration 415, loss = 0.00200596\n",
      "Iteration 416, loss = 0.00198871\n",
      "Iteration 417, loss = 0.00197163\n",
      "Iteration 418, loss = 0.00195472\n",
      "Iteration 419, loss = 0.00193802\n",
      "Iteration 420, loss = 0.00192148\n",
      "Iteration 421, loss = 0.00190508\n",
      "Iteration 422, loss = 0.00188887\n",
      "Iteration 423, loss = 0.00187283\n",
      "Iteration 424, loss = 0.00185696\n",
      "Iteration 425, loss = 0.00184124\n",
      "Iteration 426, loss = 0.00182569\n",
      "Iteration 427, loss = 0.00181029\n",
      "Iteration 428, loss = 0.00179506\n",
      "Iteration 429, loss = 0.00177999\n",
      "Iteration 430, loss = 0.00176508\n",
      "Iteration 431, loss = 0.00175030\n",
      "Iteration 432, loss = 0.00173570\n",
      "Iteration 433, loss = 0.00172125\n",
      "Iteration 434, loss = 0.00170693\n",
      "Iteration 435, loss = 0.00169278\n",
      "Iteration 436, loss = 0.00167878\n",
      "Iteration 437, loss = 0.00166493\n",
      "Iteration 438, loss = 0.00165122\n",
      "Iteration 439, loss = 0.00163765\n",
      "Iteration 440, loss = 0.00162424\n",
      "Iteration 441, loss = 0.00161099\n",
      "Iteration 442, loss = 0.00159788\n",
      "Iteration 443, loss = 0.00158492\n",
      "Iteration 444, loss = 0.00157211\n",
      "Iteration 445, loss = 0.00155942\n",
      "Iteration 446, loss = 0.00154688\n",
      "Iteration 447, loss = 0.00153448\n",
      "Iteration 448, loss = 0.00152222\n",
      "Iteration 449, loss = 0.00151009\n",
      "Iteration 450, loss = 0.00149808\n",
      "Iteration 451, loss = 0.00148620\n",
      "Iteration 452, loss = 0.00147446\n",
      "Iteration 453, loss = 0.00146285\n",
      "Iteration 454, loss = 0.00145136\n",
      "Iteration 455, loss = 0.00144001\n",
      "Iteration 456, loss = 0.00142878\n",
      "Iteration 457, loss = 0.00141767\n",
      "Iteration 458, loss = 0.00140668\n",
      "Iteration 459, loss = 0.00139582\n",
      "Iteration 460, loss = 0.00138508\n",
      "Iteration 461, loss = 0.00137445\n",
      "Iteration 462, loss = 0.00136395\n",
      "Iteration 463, loss = 0.00135357\n",
      "Iteration 464, loss = 0.00134330\n",
      "Iteration 465, loss = 0.00133313\n",
      "Iteration 466, loss = 0.00132309\n",
      "Iteration 467, loss = 0.00131315\n",
      "Iteration 468, loss = 0.00130333\n",
      "Iteration 469, loss = 0.00129363\n",
      "Iteration 470, loss = 0.00128403\n",
      "Iteration 471, loss = 0.00127453\n",
      "Iteration 472, loss = 0.00126515\n",
      "Iteration 473, loss = 0.00125587\n",
      "Iteration 474, loss = 0.00124669\n",
      "Iteration 475, loss = 0.00123762\n",
      "Iteration 476, loss = 0.00122865\n",
      "Iteration 477, loss = 0.00121979\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Training set score: 0.971763\n",
      "Test set score: -0.716126\n"
     ]
    }
   ],
   "source": [
    "# MLP regressor \n",
    "\n",
    "mlp= MLPRegressor(hidden_layer_sizes=(hidden_nodes,hidden_nodes),  activation='relu', solver='adam',    alpha=0.0001,batch_size='auto',\n",
    "               learning_rate='constant', learning_rate_init=1e-4, power_t=0.5, max_iter=10000, shuffle=True,\n",
    "               random_state=None, tol=1e-5, verbose=True, warm_start=False, momentum=0.9,\n",
    "               nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n",
    "               epsilon=1e-08)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive weight assignment\n",
    "\n",
    "wsim=y_train*0\n",
    "wnn=y_train*0\n",
    "y_predict=mlp.predict(X_train)\n",
    "\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] >y_predict[i] and y_train[i]>y_sim[i]:\n",
    "        if y_predict[i] >y_sim[i]:\n",
    "            wsim[i]=0\n",
    "            wnn[i]=1\n",
    "        else:\n",
    "            wsim[i]=1\n",
    "            wnn[i]=0\n",
    "    elif y_train[i] <y_predict[i] and y_train[i]<y_sim[i]:\n",
    "        if y_predict[i] > y_sim[i]:\n",
    "            wsim[i]=1\n",
    "            wnn[i]=0\n",
    "        else:\n",
    "            wsim[i]=0\n",
    "            wnn[i]=1\n",
    "    elif y_train[i]==y_predict[i] :\n",
    "        wsim[i]=0\n",
    "        wnn[i]=1\n",
    "    \n",
    "    elif y_train[i]==y_sim[i]:\n",
    "        wsim[i]=1\n",
    "        wnn[i]=0\n",
    "    \n",
    "    else:\n",
    "        wsim[i]=abs(y_predict[i]-y_train[i])/abs(y_predict[i]-y_sim[i])\n",
    "        wnn[i]=abs(y_sim[i]-y_train[i])/abs(y_predict[i]-y_sim[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.arange(0, len(y_train),1)\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "plt.plot(t, scaler2.inverse_transform(y_train.reshape(-1,1)), 'r--', \n",
    "         t, scaler2.inverse_transform(y_predict.reshape(-1,1)), 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.07873080\n",
      "Iteration 2, loss = 0.07820306\n",
      "Iteration 3, loss = 0.07768607\n",
      "Iteration 4, loss = 0.07718133\n",
      "Iteration 5, loss = 0.07668889\n",
      "Iteration 6, loss = 0.07620570\n",
      "Iteration 7, loss = 0.07573231\n",
      "Iteration 8, loss = 0.07526858\n",
      "Iteration 9, loss = 0.07481421\n",
      "Iteration 10, loss = 0.07435293\n",
      "Iteration 11, loss = 0.07391314\n",
      "Iteration 12, loss = 0.07348455\n",
      "Iteration 13, loss = 0.07306574\n",
      "Iteration 14, loss = 0.07265655\n",
      "Iteration 15, loss = 0.07225835\n",
      "Iteration 16, loss = 0.07186844\n",
      "Iteration 17, loss = 0.07148662\n",
      "Iteration 18, loss = 0.07111263\n",
      "Iteration 19, loss = 0.07074625\n",
      "Iteration 20, loss = 0.07038720\n",
      "Iteration 21, loss = 0.07003522\n",
      "Iteration 22, loss = 0.06969675\n",
      "Iteration 23, loss = 0.06937103\n",
      "Iteration 24, loss = 0.06905034\n",
      "Iteration 25, loss = 0.06873472\n",
      "Iteration 26, loss = 0.06842408\n",
      "Iteration 27, loss = 0.06811829\n",
      "Iteration 28, loss = 0.06781717\n",
      "Iteration 29, loss = 0.06752052\n",
      "Iteration 30, loss = 0.06722816\n",
      "Iteration 31, loss = 0.06693988\n",
      "Iteration 32, loss = 0.06665548\n",
      "Iteration 33, loss = 0.06637477\n",
      "Iteration 34, loss = 0.06609821\n",
      "Iteration 35, loss = 0.06582521\n",
      "Iteration 36, loss = 0.06555505\n",
      "Iteration 37, loss = 0.06528760\n",
      "Iteration 38, loss = 0.06502274\n",
      "Iteration 39, loss = 0.06476054\n",
      "Iteration 40, loss = 0.06450147\n",
      "Iteration 41, loss = 0.06424476\n",
      "Iteration 42, loss = 0.06399541\n",
      "Iteration 43, loss = 0.06375382\n",
      "Iteration 44, loss = 0.06351414\n",
      "Iteration 45, loss = 0.06327632\n",
      "Iteration 46, loss = 0.06304030\n",
      "Iteration 47, loss = 0.06280605\n",
      "Iteration 48, loss = 0.06257356\n",
      "Iteration 49, loss = 0.06234290\n",
      "Iteration 50, loss = 0.06211373\n",
      "Iteration 51, loss = 0.06188625\n",
      "Iteration 52, loss = 0.06166030\n",
      "Iteration 53, loss = 0.06143763\n",
      "Iteration 54, loss = 0.06121711\n",
      "Iteration 55, loss = 0.06099898\n",
      "Iteration 56, loss = 0.06078237\n",
      "Iteration 57, loss = 0.06056790\n",
      "Iteration 58, loss = 0.06035458\n",
      "Iteration 59, loss = 0.06014242\n",
      "Iteration 60, loss = 0.05993141\n",
      "Iteration 61, loss = 0.05972153\n",
      "Iteration 62, loss = 0.05951278\n",
      "Iteration 63, loss = 0.05930555\n",
      "Iteration 64, loss = 0.05910103\n",
      "Iteration 65, loss = 0.05889704\n",
      "Iteration 66, loss = 0.05869356\n",
      "Iteration 67, loss = 0.05849127\n",
      "Iteration 68, loss = 0.05829065\n",
      "Iteration 69, loss = 0.05809575\n",
      "Iteration 70, loss = 0.05790435\n",
      "Iteration 71, loss = 0.05771363\n",
      "Iteration 72, loss = 0.05752364\n",
      "Iteration 73, loss = 0.05733443\n",
      "Iteration 74, loss = 0.05714616\n",
      "Iteration 75, loss = 0.05695917\n",
      "Iteration 76, loss = 0.05677311\n",
      "Iteration 77, loss = 0.05658686\n",
      "Iteration 78, loss = 0.05639946\n",
      "Iteration 79, loss = 0.05621212\n",
      "Iteration 80, loss = 0.05602480\n",
      "Iteration 81, loss = 0.05583788\n",
      "Iteration 82, loss = 0.05565292\n",
      "Iteration 83, loss = 0.05547031\n",
      "Iteration 84, loss = 0.05528743\n",
      "Iteration 85, loss = 0.05510559\n",
      "Iteration 86, loss = 0.05492427\n",
      "Iteration 87, loss = 0.05474327\n",
      "Iteration 88, loss = 0.05456297\n",
      "Iteration 89, loss = 0.05438426\n",
      "Iteration 90, loss = 0.05420739\n",
      "Iteration 91, loss = 0.05403125\n",
      "Iteration 92, loss = 0.05385586\n",
      "Iteration 93, loss = 0.05368143\n",
      "Iteration 94, loss = 0.05350873\n",
      "Iteration 95, loss = 0.05332067\n",
      "Iteration 96, loss = 0.05313093\n",
      "Iteration 97, loss = 0.05294078\n",
      "Iteration 98, loss = 0.05274969\n",
      "Iteration 99, loss = 0.05255788\n",
      "Iteration 100, loss = 0.05236687\n",
      "Iteration 101, loss = 0.05217609\n",
      "Iteration 102, loss = 0.05198506\n",
      "Iteration 103, loss = 0.05179377\n",
      "Iteration 104, loss = 0.05160304\n",
      "Iteration 105, loss = 0.05141227\n",
      "Iteration 106, loss = 0.05122343\n",
      "Iteration 107, loss = 0.05104771\n",
      "Iteration 108, loss = 0.05090808\n",
      "Iteration 109, loss = 0.05076855\n",
      "Iteration 110, loss = 0.05062798\n",
      "Iteration 111, loss = 0.05048669\n",
      "Iteration 112, loss = 0.05034515\n",
      "Iteration 113, loss = 0.05020408\n",
      "Iteration 114, loss = 0.05006336\n",
      "Iteration 115, loss = 0.04992396\n",
      "Iteration 116, loss = 0.04978441\n",
      "Iteration 117, loss = 0.04964478\n",
      "Iteration 118, loss = 0.04950586\n",
      "Iteration 119, loss = 0.04936780\n",
      "Iteration 120, loss = 0.04923057\n",
      "Iteration 121, loss = 0.04909326\n",
      "Iteration 122, loss = 0.04895587\n",
      "Iteration 123, loss = 0.04881922\n",
      "Iteration 124, loss = 0.04868313\n",
      "Iteration 125, loss = 0.04854625\n",
      "Iteration 126, loss = 0.04840929\n",
      "Iteration 127, loss = 0.04827271\n",
      "Iteration 128, loss = 0.04813673\n",
      "Iteration 129, loss = 0.04800089\n",
      "Iteration 130, loss = 0.04786519\n",
      "Iteration 131, loss = 0.04773090\n",
      "Iteration 132, loss = 0.04760925\n",
      "Iteration 133, loss = 0.04748000\n",
      "Iteration 134, loss = 0.04734407\n",
      "Iteration 135, loss = 0.04720733\n",
      "Iteration 136, loss = 0.04708015\n",
      "Iteration 137, loss = 0.04695319\n",
      "Iteration 138, loss = 0.04682646\n",
      "Iteration 139, loss = 0.04670028\n",
      "Iteration 140, loss = 0.04657490\n",
      "Iteration 141, loss = 0.04644989\n",
      "Iteration 142, loss = 0.04632523\n",
      "Iteration 143, loss = 0.04620093\n",
      "Iteration 144, loss = 0.04607697\n",
      "Iteration 145, loss = 0.04595395\n",
      "Iteration 146, loss = 0.04583195\n",
      "Iteration 147, loss = 0.04571218\n",
      "Iteration 148, loss = 0.04559337\n",
      "Iteration 149, loss = 0.04547235\n",
      "Iteration 150, loss = 0.04535557\n",
      "Iteration 151, loss = 0.04523904\n",
      "Iteration 152, loss = 0.04512272\n",
      "Iteration 153, loss = 0.04500638\n",
      "Iteration 154, loss = 0.04488931\n",
      "Iteration 155, loss = 0.04477270\n",
      "Iteration 156, loss = 0.04465638\n",
      "Iteration 157, loss = 0.04453994\n",
      "Iteration 158, loss = 0.04442422\n",
      "Iteration 159, loss = 0.04431393\n",
      "Iteration 160, loss = 0.04420231\n",
      "Iteration 161, loss = 0.04409098\n",
      "Iteration 162, loss = 0.04397763\n",
      "Iteration 163, loss = 0.04386821\n",
      "Iteration 164, loss = 0.04375915\n",
      "Iteration 165, loss = 0.04365027\n",
      "Iteration 166, loss = 0.04354105\n",
      "Iteration 167, loss = 0.04343183\n",
      "Iteration 168, loss = 0.04332278\n",
      "Iteration 169, loss = 0.04321582\n",
      "Iteration 170, loss = 0.04311022\n",
      "Iteration 171, loss = 0.04300615\n",
      "Iteration 172, loss = 0.04290020\n",
      "Iteration 173, loss = 0.04279612\n",
      "Iteration 174, loss = 0.04269324\n",
      "Iteration 175, loss = 0.04259134\n",
      "Iteration 176, loss = 0.04248901\n",
      "Iteration 177, loss = 0.04238624\n",
      "Iteration 178, loss = 0.04228775\n",
      "Iteration 179, loss = 0.04218933\n",
      "Iteration 180, loss = 0.04208963\n",
      "Iteration 181, loss = 0.04198923\n",
      "Iteration 182, loss = 0.04189137\n",
      "Iteration 183, loss = 0.04179577\n",
      "Iteration 184, loss = 0.04170085\n",
      "Iteration 185, loss = 0.04160569\n",
      "Iteration 186, loss = 0.04151050\n",
      "Iteration 187, loss = 0.04141523\n",
      "Iteration 188, loss = 0.04132398\n",
      "Iteration 189, loss = 0.04123207\n",
      "Iteration 190, loss = 0.04113997\n",
      "Iteration 191, loss = 0.04104753\n",
      "Iteration 192, loss = 0.04095567\n",
      "Iteration 193, loss = 0.04086652\n",
      "Iteration 194, loss = 0.04077808\n",
      "Iteration 195, loss = 0.04069011\n",
      "Iteration 196, loss = 0.04060178\n",
      "Iteration 197, loss = 0.04051282\n",
      "Iteration 198, loss = 0.04042381\n",
      "Iteration 199, loss = 0.04033913\n",
      "Iteration 200, loss = 0.04025467\n",
      "Iteration 201, loss = 0.04016945\n",
      "Iteration 202, loss = 0.04008404\n",
      "Iteration 203, loss = 0.03999845\n",
      "Iteration 204, loss = 0.03991346\n",
      "Iteration 205, loss = 0.03983026\n",
      "Iteration 206, loss = 0.03974908\n",
      "Iteration 207, loss = 0.03966782\n",
      "Iteration 208, loss = 0.03958571\n",
      "Iteration 209, loss = 0.03950363\n",
      "Iteration 210, loss = 0.03942429\n",
      "Iteration 211, loss = 0.03934595\n",
      "Iteration 212, loss = 0.03926745\n",
      "Iteration 213, loss = 0.03918879\n",
      "Iteration 214, loss = 0.03911015\n",
      "Iteration 215, loss = 0.03903210\n",
      "Iteration 216, loss = 0.03895475\n",
      "Iteration 217, loss = 0.03887949\n",
      "Iteration 218, loss = 0.03880406\n",
      "Iteration 219, loss = 0.03872941\n",
      "Iteration 220, loss = 0.03865514\n",
      "Iteration 221, loss = 0.03858131\n",
      "Iteration 222, loss = 0.03850838\n",
      "Iteration 223, loss = 0.03843766\n",
      "Iteration 224, loss = 0.03836574\n",
      "Iteration 225, loss = 0.03829336\n",
      "Iteration 226, loss = 0.03822390\n",
      "Iteration 227, loss = 0.03815748\n",
      "Iteration 228, loss = 0.03809089\n",
      "Iteration 229, loss = 0.03802497\n",
      "Iteration 230, loss = 0.03795972\n",
      "Iteration 231, loss = 0.03789513\n",
      "Iteration 232, loss = 0.03783042\n",
      "Iteration 233, loss = 0.03776746\n",
      "Iteration 234, loss = 0.03770476\n",
      "Iteration 235, loss = 0.03764168\n",
      "Iteration 236, loss = 0.03758074\n",
      "Iteration 237, loss = 0.03751981\n",
      "Iteration 238, loss = 0.03745865\n",
      "Iteration 239, loss = 0.03739926\n",
      "Iteration 240, loss = 0.03733986\n",
      "Iteration 241, loss = 0.03728061\n",
      "Iteration 242, loss = 0.03722192\n",
      "Iteration 243, loss = 0.03716352\n",
      "Iteration 244, loss = 0.03710648\n",
      "Iteration 245, loss = 0.03704931\n",
      "Iteration 246, loss = 0.03699224\n",
      "Iteration 247, loss = 0.03693620\n",
      "Iteration 248, loss = 0.03688110\n",
      "Iteration 249, loss = 0.03682615\n",
      "Iteration 250, loss = 0.03677102\n",
      "Iteration 251, loss = 0.03671584\n",
      "Iteration 252, loss = 0.03666169\n",
      "Iteration 253, loss = 0.03660872\n",
      "Iteration 254, loss = 0.03655566\n",
      "Iteration 255, loss = 0.03650234\n",
      "Iteration 256, loss = 0.03644966\n",
      "Iteration 257, loss = 0.03639812\n",
      "Iteration 258, loss = 0.03634650\n",
      "Iteration 259, loss = 0.03629568\n",
      "Iteration 260, loss = 0.03624502\n",
      "Iteration 261, loss = 0.03619456\n",
      "Iteration 262, loss = 0.03614449\n",
      "Iteration 263, loss = 0.03609497\n",
      "Iteration 264, loss = 0.03604618\n",
      "Iteration 265, loss = 0.03599680\n",
      "Iteration 266, loss = 0.03594861\n",
      "Iteration 267, loss = 0.03590105\n",
      "Iteration 268, loss = 0.03585321\n",
      "Iteration 269, loss = 0.03580538\n",
      "Iteration 270, loss = 0.03575835\n",
      "Iteration 271, loss = 0.03571149\n",
      "Iteration 272, loss = 0.03566509\n",
      "Iteration 273, loss = 0.03561907\n",
      "Iteration 274, loss = 0.03557293\n",
      "Iteration 275, loss = 0.03552742\n",
      "Iteration 276, loss = 0.03548208\n",
      "Iteration 277, loss = 0.03543718\n",
      "Iteration 278, loss = 0.03539251\n",
      "Iteration 279, loss = 0.03534791\n",
      "Iteration 280, loss = 0.03530390\n",
      "Iteration 281, loss = 0.03526017\n",
      "Iteration 282, loss = 0.03521684\n",
      "Iteration 283, loss = 0.03517362\n",
      "Iteration 284, loss = 0.03513088\n",
      "Iteration 285, loss = 0.03508836\n",
      "Iteration 286, loss = 0.03504606\n",
      "Iteration 287, loss = 0.03500410\n",
      "Iteration 288, loss = 0.03496238\n",
      "Iteration 289, loss = 0.03492160\n",
      "Iteration 290, loss = 0.03488022\n",
      "Iteration 291, loss = 0.03483980\n",
      "Iteration 292, loss = 0.03479978\n",
      "Iteration 293, loss = 0.03476088\n",
      "Iteration 294, loss = 0.03472049\n",
      "Iteration 295, loss = 0.03468130\n",
      "Iteration 296, loss = 0.03464243\n",
      "Iteration 297, loss = 0.03460394\n",
      "Iteration 298, loss = 0.03456555\n",
      "Iteration 299, loss = 0.03452723\n",
      "Iteration 300, loss = 0.03448950\n",
      "Iteration 301, loss = 0.03445203\n",
      "Iteration 302, loss = 0.03441498\n",
      "Iteration 303, loss = 0.03437790\n",
      "Iteration 304, loss = 0.03434129\n",
      "Iteration 305, loss = 0.03430513\n",
      "Iteration 306, loss = 0.03426893\n",
      "Iteration 307, loss = 0.03423348\n",
      "Iteration 308, loss = 0.03419796\n",
      "Iteration 309, loss = 0.03416240\n",
      "Iteration 310, loss = 0.03412739\n",
      "Iteration 311, loss = 0.03409269\n",
      "Iteration 312, loss = 0.03405816\n",
      "Iteration 313, loss = 0.03402377\n",
      "Iteration 314, loss = 0.03398976\n",
      "Iteration 315, loss = 0.03395596\n",
      "Iteration 316, loss = 0.03392248\n",
      "Iteration 317, loss = 0.03388915\n",
      "Iteration 318, loss = 0.03385612\n",
      "Iteration 319, loss = 0.03382372\n",
      "Iteration 320, loss = 0.03379196\n",
      "Iteration 321, loss = 0.03376049\n",
      "Iteration 322, loss = 0.03372930\n",
      "Iteration 323, loss = 0.03369823\n",
      "Iteration 324, loss = 0.03366742\n",
      "Iteration 325, loss = 0.03363671\n",
      "Iteration 326, loss = 0.03360619\n",
      "Iteration 327, loss = 0.03357591\n",
      "Iteration 328, loss = 0.03354595\n",
      "Iteration 329, loss = 0.03351622\n",
      "Iteration 330, loss = 0.03348648\n",
      "Iteration 331, loss = 0.03345695\n",
      "Iteration 332, loss = 0.03342770\n",
      "Iteration 333, loss = 0.03339854\n",
      "Iteration 334, loss = 0.03336978\n",
      "Iteration 335, loss = 0.03334102\n",
      "Iteration 336, loss = 0.03331245\n",
      "Iteration 337, loss = 0.03328398\n",
      "Iteration 338, loss = 0.03325595\n",
      "Iteration 339, loss = 0.03322799\n",
      "Iteration 340, loss = 0.03320007\n",
      "Iteration 341, loss = 0.03317222\n",
      "Iteration 342, loss = 0.03314472\n",
      "Iteration 343, loss = 0.03311731\n",
      "Iteration 344, loss = 0.03308992\n",
      "Iteration 345, loss = 0.03306262\n",
      "Iteration 346, loss = 0.03303541\n",
      "Iteration 347, loss = 0.03300834\n",
      "Iteration 348, loss = 0.03298133\n",
      "Iteration 349, loss = 0.03295512\n",
      "Iteration 350, loss = 0.03293069\n",
      "Iteration 351, loss = 0.03290617\n",
      "Iteration 352, loss = 0.03288155\n",
      "Iteration 353, loss = 0.03285722\n",
      "Iteration 354, loss = 0.03283305\n",
      "Iteration 355, loss = 0.03280882\n",
      "Iteration 356, loss = 0.03278482\n",
      "Iteration 357, loss = 0.03276096\n",
      "Iteration 358, loss = 0.03273710\n",
      "Iteration 359, loss = 0.03271340\n",
      "Iteration 360, loss = 0.03268992\n",
      "Iteration 361, loss = 0.03266637\n",
      "Iteration 362, loss = 0.03264283\n",
      "Iteration 363, loss = 0.03261948\n",
      "Iteration 364, loss = 0.03259628\n",
      "Iteration 365, loss = 0.03257305\n",
      "Iteration 366, loss = 0.03255003\n",
      "Iteration 367, loss = 0.03252708\n",
      "Iteration 368, loss = 0.03250420\n",
      "Iteration 369, loss = 0.03248146\n",
      "Iteration 370, loss = 0.03245870\n",
      "Iteration 371, loss = 0.03243605\n",
      "Iteration 372, loss = 0.03241363\n",
      "Iteration 373, loss = 0.03239121\n",
      "Iteration 374, loss = 0.03236887\n",
      "Iteration 375, loss = 0.03234617\n",
      "Iteration 376, loss = 0.03232316\n",
      "Iteration 377, loss = 0.03229993\n",
      "Iteration 378, loss = 0.03227674\n",
      "Iteration 379, loss = 0.03225351\n",
      "Iteration 380, loss = 0.03223017\n",
      "Iteration 381, loss = 0.03220688\n",
      "Iteration 382, loss = 0.03218550\n",
      "Iteration 383, loss = 0.03216395\n",
      "Iteration 384, loss = 0.03214222\n",
      "Iteration 385, loss = 0.03212083\n",
      "Iteration 386, loss = 0.03209954\n",
      "Iteration 387, loss = 0.03207819\n",
      "Iteration 388, loss = 0.03205676\n",
      "Iteration 389, loss = 0.03203531\n",
      "Iteration 390, loss = 0.03201479\n",
      "Iteration 391, loss = 0.03199426\n",
      "Iteration 392, loss = 0.03197270\n",
      "Iteration 393, loss = 0.03195181\n",
      "Iteration 394, loss = 0.03193126\n",
      "Iteration 395, loss = 0.03191062\n",
      "Iteration 396, loss = 0.03188992\n",
      "Iteration 397, loss = 0.03186945\n",
      "Iteration 398, loss = 0.03184892\n",
      "Iteration 399, loss = 0.03182825\n",
      "Iteration 400, loss = 0.03180789\n",
      "Iteration 401, loss = 0.03178803\n",
      "Iteration 402, loss = 0.03176727\n",
      "Iteration 403, loss = 0.03174703\n",
      "Iteration 404, loss = 0.03172702\n",
      "Iteration 405, loss = 0.03170694\n",
      "Iteration 406, loss = 0.03168682\n",
      "Iteration 407, loss = 0.03166668\n",
      "Iteration 408, loss = 0.03164672\n",
      "Iteration 409, loss = 0.03162757\n",
      "Iteration 410, loss = 0.03160812\n",
      "Iteration 411, loss = 0.03158836\n",
      "Iteration 412, loss = 0.03156882\n",
      "Iteration 413, loss = 0.03154943\n",
      "Iteration 414, loss = 0.03152985\n",
      "Iteration 415, loss = 0.03151048\n",
      "Iteration 416, loss = 0.03149101\n",
      "Iteration 417, loss = 0.03147172\n",
      "Iteration 418, loss = 0.03145219\n",
      "Iteration 419, loss = 0.03143301\n",
      "Iteration 420, loss = 0.03141393\n",
      "Iteration 421, loss = 0.03139450\n",
      "Iteration 422, loss = 0.03137534\n",
      "Iteration 423, loss = 0.03135637\n",
      "Iteration 424, loss = 0.03133700\n",
      "Iteration 425, loss = 0.03131773\n",
      "Iteration 426, loss = 0.03129874\n",
      "Iteration 427, loss = 0.03127944\n",
      "Iteration 428, loss = 0.03125973\n",
      "Iteration 429, loss = 0.03124037\n",
      "Iteration 430, loss = 0.03122112\n",
      "Iteration 431, loss = 0.03120177\n",
      "Iteration 432, loss = 0.03118225\n",
      "Iteration 433, loss = 0.03116279\n",
      "Iteration 434, loss = 0.03114314\n",
      "Iteration 435, loss = 0.03112370\n",
      "Iteration 436, loss = 0.03110398\n",
      "Iteration 437, loss = 0.03108447\n",
      "Iteration 438, loss = 0.03106523\n",
      "Iteration 439, loss = 0.03104544\n",
      "Iteration 440, loss = 0.03102582\n",
      "Iteration 441, loss = 0.03100611\n",
      "Iteration 442, loss = 0.03098638\n",
      "Iteration 443, loss = 0.03096658\n",
      "Iteration 444, loss = 0.03094698\n",
      "Iteration 445, loss = 0.03092757\n",
      "Iteration 446, loss = 0.03090742\n",
      "Iteration 447, loss = 0.03088782\n",
      "Iteration 448, loss = 0.03086818\n",
      "Iteration 449, loss = 0.03084839\n",
      "Iteration 450, loss = 0.03082868\n",
      "Iteration 451, loss = 0.03080891\n",
      "Iteration 452, loss = 0.03079012\n",
      "Iteration 453, loss = 0.03077100\n",
      "Iteration 454, loss = 0.03075210\n",
      "Iteration 455, loss = 0.03073317\n",
      "Iteration 456, loss = 0.03071427\n",
      "Iteration 457, loss = 0.03069528\n",
      "Iteration 458, loss = 0.03067616\n",
      "Iteration 459, loss = 0.03065770\n",
      "Iteration 460, loss = 0.03063826\n",
      "Iteration 461, loss = 0.03061952\n",
      "Iteration 462, loss = 0.03060054\n",
      "Iteration 463, loss = 0.03058154\n",
      "Iteration 464, loss = 0.03056253\n",
      "Iteration 465, loss = 0.03054362\n",
      "Iteration 466, loss = 0.03052747\n",
      "Iteration 467, loss = 0.03051079\n",
      "Iteration 468, loss = 0.03049237\n",
      "Iteration 469, loss = 0.03047294\n",
      "Iteration 470, loss = 0.03045320\n",
      "Iteration 471, loss = 0.03043518\n",
      "Iteration 472, loss = 0.03041751\n",
      "Iteration 473, loss = 0.03040035\n",
      "Iteration 474, loss = 0.03038210\n",
      "Iteration 475, loss = 0.03036437\n",
      "Iteration 476, loss = 0.03034637\n",
      "Iteration 477, loss = 0.03032836\n",
      "Iteration 478, loss = 0.03031010\n",
      "Iteration 479, loss = 0.03029233\n",
      "Iteration 480, loss = 0.03027353\n",
      "Iteration 481, loss = 0.03025514\n",
      "Iteration 482, loss = 0.03023657\n",
      "Iteration 483, loss = 0.03022007\n",
      "Iteration 484, loss = 0.03020231\n",
      "Iteration 485, loss = 0.03018353\n",
      "Iteration 486, loss = 0.03016496\n",
      "Iteration 487, loss = 0.03014740\n",
      "Iteration 488, loss = 0.03012979\n",
      "Iteration 489, loss = 0.03011193\n",
      "Iteration 490, loss = 0.03009459\n",
      "Iteration 491, loss = 0.03007730\n",
      "Iteration 492, loss = 0.03005959\n",
      "Iteration 493, loss = 0.03004130\n",
      "Iteration 494, loss = 0.03002301\n",
      "Iteration 495, loss = 0.03000473\n",
      "Iteration 496, loss = 0.02998683\n",
      "Iteration 497, loss = 0.02996980\n",
      "Iteration 498, loss = 0.02995181\n",
      "Iteration 499, loss = 0.02993450\n",
      "Iteration 500, loss = 0.02991706\n",
      "Iteration 501, loss = 0.02989945\n",
      "Iteration 502, loss = 0.02988155\n",
      "Iteration 503, loss = 0.02986470\n",
      "Iteration 504, loss = 0.02984807\n",
      "Iteration 505, loss = 0.02982917\n",
      "Iteration 506, loss = 0.02981182\n",
      "Iteration 507, loss = 0.02979496\n",
      "Iteration 508, loss = 0.02977795\n",
      "Iteration 509, loss = 0.02976064\n",
      "Iteration 510, loss = 0.02974336\n",
      "Iteration 511, loss = 0.02972591\n",
      "Iteration 512, loss = 0.02970813\n",
      "Iteration 513, loss = 0.02969027\n",
      "Iteration 514, loss = 0.02967287\n",
      "Iteration 515, loss = 0.02965542\n",
      "Iteration 516, loss = 0.02963795\n",
      "Iteration 517, loss = 0.02962065\n",
      "Iteration 518, loss = 0.02960322\n",
      "Iteration 519, loss = 0.02958608\n",
      "Iteration 520, loss = 0.02956825\n",
      "Iteration 521, loss = 0.02955158\n",
      "Iteration 522, loss = 0.02953367\n",
      "Iteration 523, loss = 0.02951686\n",
      "Iteration 524, loss = 0.02949974\n",
      "Iteration 525, loss = 0.02948237\n",
      "Iteration 526, loss = 0.02946582\n",
      "Iteration 527, loss = 0.02944795\n",
      "Iteration 528, loss = 0.02943117\n",
      "Iteration 529, loss = 0.02941340\n",
      "Iteration 530, loss = 0.02939647\n",
      "Iteration 531, loss = 0.02937937\n",
      "Iteration 532, loss = 0.02936221\n",
      "Iteration 533, loss = 0.02934515\n",
      "Iteration 534, loss = 0.02932805\n",
      "Iteration 535, loss = 0.02931062\n",
      "Iteration 536, loss = 0.02929304\n",
      "Iteration 537, loss = 0.02927520\n",
      "Iteration 538, loss = 0.02925915\n",
      "Iteration 539, loss = 0.02924184\n",
      "Iteration 540, loss = 0.02922461\n",
      "Iteration 541, loss = 0.02920695\n",
      "Iteration 542, loss = 0.02918976\n",
      "Iteration 543, loss = 0.02917297\n",
      "Iteration 544, loss = 0.02915589\n",
      "Iteration 545, loss = 0.02913880\n",
      "Iteration 546, loss = 0.02912147\n",
      "Iteration 547, loss = 0.02910501\n",
      "Iteration 548, loss = 0.02908780\n",
      "Iteration 549, loss = 0.02907035\n",
      "Iteration 550, loss = 0.02905338\n",
      "Iteration 551, loss = 0.02903637\n",
      "Iteration 552, loss = 0.02901904\n",
      "Iteration 553, loss = 0.02900187\n",
      "Iteration 554, loss = 0.02898488\n",
      "Iteration 555, loss = 0.02896771\n",
      "Iteration 556, loss = 0.02894988\n",
      "Iteration 557, loss = 0.02893310\n",
      "Iteration 558, loss = 0.02891794\n",
      "Iteration 559, loss = 0.02890214\n",
      "Iteration 560, loss = 0.02888773\n",
      "Iteration 561, loss = 0.02887198\n",
      "Iteration 562, loss = 0.02885543\n",
      "Iteration 563, loss = 0.02884014\n",
      "Iteration 564, loss = 0.02882450\n",
      "Iteration 565, loss = 0.02880879\n",
      "Iteration 566, loss = 0.02879326\n",
      "Iteration 567, loss = 0.02877802\n",
      "Iteration 568, loss = 0.02876181\n",
      "Iteration 569, loss = 0.02874713\n",
      "Iteration 570, loss = 0.02873329\n",
      "Iteration 571, loss = 0.02871845\n",
      "Iteration 572, loss = 0.02870312\n",
      "Iteration 573, loss = 0.02868764\n",
      "Iteration 574, loss = 0.02867331\n",
      "Iteration 575, loss = 0.02865767\n",
      "Iteration 576, loss = 0.02864276\n",
      "Iteration 577, loss = 0.02862906\n",
      "Iteration 578, loss = 0.02861297\n",
      "Iteration 579, loss = 0.02859989\n",
      "Iteration 580, loss = 0.02858580\n",
      "Iteration 581, loss = 0.02857111\n",
      "Iteration 582, loss = 0.02855563\n",
      "Iteration 583, loss = 0.02853973\n",
      "Iteration 584, loss = 0.02852407\n",
      "Iteration 585, loss = 0.02851229\n",
      "Iteration 586, loss = 0.02849638\n",
      "Iteration 587, loss = 0.02848026\n",
      "Iteration 588, loss = 0.02846650\n",
      "Iteration 589, loss = 0.02845291\n",
      "Iteration 590, loss = 0.02843894\n",
      "Iteration 591, loss = 0.02842418\n",
      "Iteration 592, loss = 0.02840867\n",
      "Iteration 593, loss = 0.02839415\n",
      "Iteration 594, loss = 0.02837962\n",
      "Iteration 595, loss = 0.02836568\n",
      "Iteration 596, loss = 0.02834821\n",
      "Iteration 597, loss = 0.02833111\n",
      "Iteration 598, loss = 0.02831460\n",
      "Iteration 599, loss = 0.02829928\n",
      "Iteration 600, loss = 0.02828121\n",
      "Iteration 601, loss = 0.02826246\n",
      "Iteration 602, loss = 0.02824386\n",
      "Iteration 603, loss = 0.02822509\n",
      "Iteration 604, loss = 0.02820660\n",
      "Iteration 605, loss = 0.02819130\n",
      "Iteration 606, loss = 0.02817169\n",
      "Iteration 607, loss = 0.02815077\n",
      "Iteration 608, loss = 0.02813238\n",
      "Iteration 609, loss = 0.02811351\n",
      "Iteration 610, loss = 0.02809419\n",
      "Iteration 611, loss = 0.02807555\n",
      "Iteration 612, loss = 0.02805499\n",
      "Iteration 613, loss = 0.02803680\n",
      "Iteration 614, loss = 0.02801695\n",
      "Iteration 615, loss = 0.02799857\n",
      "Iteration 616, loss = 0.02797851\n",
      "Iteration 617, loss = 0.02795985\n",
      "Iteration 618, loss = 0.02793912\n",
      "Iteration 619, loss = 0.02792113\n",
      "Iteration 620, loss = 0.02790204\n",
      "Iteration 621, loss = 0.02788321\n",
      "Iteration 622, loss = 0.02786379\n",
      "Iteration 623, loss = 0.02784271\n",
      "Iteration 624, loss = 0.02782114\n",
      "Iteration 625, loss = 0.02780219\n",
      "Iteration 626, loss = 0.02778312\n",
      "Iteration 627, loss = 0.02776434\n",
      "Iteration 628, loss = 0.02774437\n",
      "Iteration 629, loss = 0.02772502\n",
      "Iteration 630, loss = 0.02770476\n",
      "Iteration 631, loss = 0.02768650\n",
      "Iteration 632, loss = 0.02766577\n",
      "Iteration 633, loss = 0.02764894\n",
      "Iteration 634, loss = 0.02763336\n",
      "Iteration 635, loss = 0.02761698\n",
      "Iteration 636, loss = 0.02760216\n",
      "Iteration 637, loss = 0.02758641\n",
      "Iteration 638, loss = 0.02757017\n",
      "Iteration 639, loss = 0.02755497\n",
      "Iteration 640, loss = 0.02754036\n",
      "Iteration 641, loss = 0.02752526\n",
      "Iteration 642, loss = 0.02750947\n",
      "Iteration 643, loss = 0.02749449\n",
      "Iteration 644, loss = 0.02747953\n",
      "Iteration 645, loss = 0.02746440\n",
      "Iteration 646, loss = 0.02744956\n",
      "Iteration 647, loss = 0.02743427\n",
      "Iteration 648, loss = 0.02741918\n",
      "Iteration 649, loss = 0.02740420\n",
      "Iteration 650, loss = 0.02738705\n",
      "Iteration 651, loss = 0.02736939\n",
      "Iteration 652, loss = 0.02735092\n",
      "Iteration 653, loss = 0.02733179\n",
      "Iteration 654, loss = 0.02731142\n",
      "Iteration 655, loss = 0.02729173\n",
      "Iteration 656, loss = 0.02727264\n",
      "Iteration 657, loss = 0.02725252\n",
      "Iteration 658, loss = 0.02723150\n",
      "Iteration 659, loss = 0.02721068\n",
      "Iteration 660, loss = 0.02719039\n",
      "Iteration 661, loss = 0.02717011\n",
      "Iteration 662, loss = 0.02714858\n",
      "Iteration 663, loss = 0.02712774\n",
      "Iteration 664, loss = 0.02711068\n",
      "Iteration 665, loss = 0.02709442\n",
      "Iteration 666, loss = 0.02707689\n",
      "Iteration 667, loss = 0.02705917\n",
      "Iteration 668, loss = 0.02704210\n",
      "Iteration 669, loss = 0.02702773\n",
      "Iteration 670, loss = 0.02700966\n",
      "Iteration 671, loss = 0.02699307\n",
      "Iteration 672, loss = 0.02697753\n",
      "Iteration 673, loss = 0.02696224\n",
      "Iteration 674, loss = 0.02694576\n",
      "Iteration 675, loss = 0.02692793\n",
      "Iteration 676, loss = 0.02691078\n",
      "Iteration 677, loss = 0.02689428\n",
      "Iteration 678, loss = 0.02687981\n",
      "Iteration 679, loss = 0.02686146\n",
      "Iteration 680, loss = 0.02684544\n",
      "Iteration 681, loss = 0.02682916\n",
      "Iteration 682, loss = 0.02681300\n",
      "Iteration 683, loss = 0.02679739\n",
      "Iteration 684, loss = 0.02678195\n",
      "Iteration 685, loss = 0.02676593\n",
      "Iteration 686, loss = 0.02674994\n",
      "Iteration 687, loss = 0.02673363\n",
      "Iteration 688, loss = 0.02671992\n",
      "Iteration 689, loss = 0.02670294\n",
      "Iteration 690, loss = 0.02668700\n",
      "Iteration 691, loss = 0.02667128\n",
      "Iteration 692, loss = 0.02665637\n",
      "Iteration 693, loss = 0.02663932\n",
      "Iteration 694, loss = 0.02662307\n",
      "Iteration 695, loss = 0.02660878\n",
      "Iteration 696, loss = 0.02659596\n",
      "Iteration 697, loss = 0.02658258\n",
      "Iteration 698, loss = 0.02656902\n",
      "Iteration 699, loss = 0.02655407\n",
      "Iteration 700, loss = 0.02653756\n",
      "Iteration 701, loss = 0.02652091\n",
      "Iteration 702, loss = 0.02650408\n",
      "Iteration 703, loss = 0.02649143\n",
      "Iteration 704, loss = 0.02647740\n",
      "Iteration 705, loss = 0.02646298\n",
      "Iteration 706, loss = 0.02644777\n",
      "Iteration 707, loss = 0.02643265\n",
      "Iteration 708, loss = 0.02641817\n",
      "Iteration 709, loss = 0.02640305\n",
      "Iteration 710, loss = 0.02638877\n",
      "Iteration 711, loss = 0.02637376\n",
      "Iteration 712, loss = 0.02635926\n",
      "Iteration 713, loss = 0.02634418\n",
      "Iteration 714, loss = 0.02632881\n",
      "Iteration 715, loss = 0.02631496\n",
      "Iteration 716, loss = 0.02630038\n",
      "Iteration 717, loss = 0.02628538\n",
      "Iteration 718, loss = 0.02627171\n",
      "Iteration 719, loss = 0.02625721\n",
      "Iteration 720, loss = 0.02624178\n",
      "Iteration 721, loss = 0.02622837\n",
      "Iteration 722, loss = 0.02621282\n",
      "Iteration 723, loss = 0.02619972\n",
      "Iteration 724, loss = 0.02618569\n",
      "Iteration 725, loss = 0.02617158\n",
      "Iteration 726, loss = 0.02615720\n",
      "Iteration 727, loss = 0.02614260\n",
      "Iteration 728, loss = 0.02612746\n",
      "Iteration 729, loss = 0.02611252\n",
      "Iteration 730, loss = 0.02610117\n",
      "Iteration 731, loss = 0.02608608\n",
      "Iteration 732, loss = 0.02606992\n",
      "Iteration 733, loss = 0.02605637\n",
      "Iteration 734, loss = 0.02604158\n",
      "Iteration 735, loss = 0.02602737\n",
      "Iteration 736, loss = 0.02601374\n",
      "Iteration 737, loss = 0.02599952\n",
      "Iteration 738, loss = 0.02598424\n",
      "Iteration 739, loss = 0.02596989\n",
      "Iteration 740, loss = 0.02595551\n",
      "Iteration 741, loss = 0.02594226\n",
      "Iteration 742, loss = 0.02592833\n",
      "Iteration 743, loss = 0.02591603\n",
      "Iteration 744, loss = 0.02590219\n",
      "Iteration 745, loss = 0.02588968\n",
      "Iteration 746, loss = 0.02587763\n",
      "Iteration 747, loss = 0.02586450\n",
      "Iteration 748, loss = 0.02585255\n",
      "Iteration 749, loss = 0.02583963\n",
      "Iteration 750, loss = 0.02582627\n",
      "Iteration 751, loss = 0.02581554\n",
      "Iteration 752, loss = 0.02580257\n",
      "Iteration 753, loss = 0.02578855\n",
      "Iteration 754, loss = 0.02577492\n",
      "Iteration 755, loss = 0.02576310\n",
      "Iteration 756, loss = 0.02575040\n",
      "Iteration 757, loss = 0.02573780\n",
      "Iteration 758, loss = 0.02572363\n",
      "Iteration 759, loss = 0.02571063\n",
      "Iteration 760, loss = 0.02569956\n",
      "Iteration 761, loss = 0.02568639\n",
      "Iteration 762, loss = 0.02567373\n",
      "Iteration 763, loss = 0.02566107\n",
      "Iteration 764, loss = 0.02564812\n",
      "Iteration 765, loss = 0.02563461\n",
      "Iteration 766, loss = 0.02562276\n",
      "Iteration 767, loss = 0.02561062\n",
      "Iteration 768, loss = 0.02559814\n",
      "Iteration 769, loss = 0.02558509\n",
      "Iteration 770, loss = 0.02557230\n",
      "Iteration 771, loss = 0.02556051\n",
      "Iteration 772, loss = 0.02554790\n",
      "Iteration 773, loss = 0.02553458\n",
      "Iteration 774, loss = 0.02552234\n",
      "Iteration 775, loss = 0.02550997\n",
      "Iteration 776, loss = 0.02550042\n",
      "Iteration 777, loss = 0.02548621\n",
      "Iteration 778, loss = 0.02547255\n",
      "Iteration 779, loss = 0.02546163\n",
      "Iteration 780, loss = 0.02544943\n",
      "Iteration 781, loss = 0.02543737\n",
      "Iteration 782, loss = 0.02542479\n",
      "Iteration 783, loss = 0.02541192\n",
      "Iteration 784, loss = 0.02539935\n",
      "Iteration 785, loss = 0.02538655\n",
      "Iteration 786, loss = 0.02537406\n",
      "Iteration 787, loss = 0.02536122\n",
      "Iteration 788, loss = 0.02535018\n",
      "Iteration 789, loss = 0.02533781\n",
      "Iteration 790, loss = 0.02532461\n",
      "Iteration 791, loss = 0.02531190\n",
      "Iteration 792, loss = 0.02530054\n",
      "Iteration 793, loss = 0.02528667\n",
      "Iteration 794, loss = 0.02527472\n",
      "Iteration 795, loss = 0.02526226\n",
      "Iteration 796, loss = 0.02524951\n",
      "Iteration 797, loss = 0.02523722\n",
      "Iteration 798, loss = 0.02522507\n",
      "Iteration 799, loss = 0.02521235\n",
      "Iteration 800, loss = 0.02520067\n",
      "Iteration 801, loss = 0.02518651\n",
      "Iteration 802, loss = 0.02517522\n",
      "Iteration 803, loss = 0.02516463\n",
      "Iteration 804, loss = 0.02515261\n",
      "Iteration 805, loss = 0.02513866\n",
      "Iteration 806, loss = 0.02512703\n",
      "Iteration 807, loss = 0.02511528\n",
      "Iteration 808, loss = 0.02510304\n",
      "Iteration 809, loss = 0.02509234\n",
      "Iteration 810, loss = 0.02507923\n",
      "Iteration 811, loss = 0.02506551\n",
      "Iteration 812, loss = 0.02505523\n",
      "Iteration 813, loss = 0.02504420\n",
      "Iteration 814, loss = 0.02503200\n",
      "Iteration 815, loss = 0.02501847\n",
      "Iteration 816, loss = 0.02500489\n",
      "Iteration 817, loss = 0.02499341\n",
      "Iteration 818, loss = 0.02498188\n",
      "Iteration 819, loss = 0.02496965\n",
      "Iteration 820, loss = 0.02495769\n",
      "Iteration 821, loss = 0.02494522\n",
      "Iteration 822, loss = 0.02493362\n",
      "Iteration 823, loss = 0.02492260\n",
      "Iteration 824, loss = 0.02491004\n",
      "Iteration 825, loss = 0.02489794\n",
      "Iteration 826, loss = 0.02488557\n",
      "Iteration 827, loss = 0.02487467\n",
      "Iteration 828, loss = 0.02486435\n",
      "Iteration 829, loss = 0.02485285\n",
      "Iteration 830, loss = 0.02484012\n",
      "Iteration 831, loss = 0.02482729\n",
      "Iteration 832, loss = 0.02481580\n",
      "Iteration 833, loss = 0.02480404\n",
      "Iteration 834, loss = 0.02479034\n",
      "Iteration 835, loss = 0.02478095\n",
      "Iteration 836, loss = 0.02476986\n",
      "Iteration 837, loss = 0.02475525\n",
      "Iteration 838, loss = 0.02474388\n",
      "Iteration 839, loss = 0.02473145\n",
      "Iteration 840, loss = 0.02472017\n",
      "Iteration 841, loss = 0.02470908\n",
      "Iteration 842, loss = 0.02469804\n",
      "Iteration 843, loss = 0.02468435\n",
      "Iteration 844, loss = 0.02467412\n",
      "Iteration 845, loss = 0.02466350\n",
      "Iteration 846, loss = 0.02465237\n",
      "Iteration 847, loss = 0.02464045\n",
      "Iteration 848, loss = 0.02462819\n",
      "Iteration 849, loss = 0.02461509\n",
      "Iteration 850, loss = 0.02460184\n",
      "Iteration 851, loss = 0.02459099\n",
      "Iteration 852, loss = 0.02458018\n",
      "Iteration 853, loss = 0.02456927\n",
      "Iteration 854, loss = 0.02455675\n",
      "Iteration 855, loss = 0.02454323\n",
      "Iteration 856, loss = 0.02453275\n",
      "Iteration 857, loss = 0.02452156\n",
      "Iteration 858, loss = 0.02451019\n",
      "Iteration 859, loss = 0.02449829\n",
      "Iteration 860, loss = 0.02448642\n",
      "Iteration 861, loss = 0.02447509\n",
      "Iteration 862, loss = 0.02446228\n",
      "Iteration 863, loss = 0.02444984\n",
      "Iteration 864, loss = 0.02443902\n",
      "Iteration 865, loss = 0.02442716\n",
      "Iteration 866, loss = 0.02441492\n",
      "Iteration 867, loss = 0.02440375\n",
      "Iteration 868, loss = 0.02439271\n",
      "Iteration 869, loss = 0.02438073\n",
      "Iteration 870, loss = 0.02436950\n",
      "Iteration 871, loss = 0.02435760\n",
      "Iteration 872, loss = 0.02434549\n",
      "Iteration 873, loss = 0.02433507\n",
      "Iteration 874, loss = 0.02432328\n",
      "Iteration 875, loss = 0.02431219\n",
      "Iteration 876, loss = 0.02429968\n",
      "Iteration 877, loss = 0.02428803\n",
      "Iteration 878, loss = 0.02427750\n",
      "Iteration 879, loss = 0.02426561\n",
      "Iteration 880, loss = 0.02425376\n",
      "Iteration 881, loss = 0.02424397\n",
      "Iteration 882, loss = 0.02423248\n",
      "Iteration 883, loss = 0.02422078\n",
      "Iteration 884, loss = 0.02420900\n",
      "Iteration 885, loss = 0.02419851\n",
      "Iteration 886, loss = 0.02418714\n",
      "Iteration 887, loss = 0.02417552\n",
      "Iteration 888, loss = 0.02416484\n",
      "Iteration 889, loss = 0.02415369\n",
      "Iteration 890, loss = 0.02414136\n",
      "Iteration 891, loss = 0.02413284\n",
      "Iteration 892, loss = 0.02412072\n",
      "Iteration 893, loss = 0.02410812\n",
      "Iteration 894, loss = 0.02409791\n",
      "Iteration 895, loss = 0.02408661\n",
      "Iteration 896, loss = 0.02407589\n",
      "Iteration 897, loss = 0.02406513\n",
      "Iteration 898, loss = 0.02405381\n",
      "Iteration 899, loss = 0.02404206\n",
      "Iteration 900, loss = 0.02403240\n",
      "Iteration 901, loss = 0.02402170\n",
      "Iteration 902, loss = 0.02400913\n",
      "Iteration 903, loss = 0.02399867\n",
      "Iteration 904, loss = 0.02398763\n",
      "Iteration 905, loss = 0.02397681\n",
      "Iteration 906, loss = 0.02396512\n",
      "Iteration 907, loss = 0.02395374\n",
      "Iteration 908, loss = 0.02394395\n",
      "Iteration 909, loss = 0.02393315\n",
      "Iteration 910, loss = 0.02392175\n",
      "Iteration 911, loss = 0.02391085\n",
      "Iteration 912, loss = 0.02390022\n",
      "Iteration 913, loss = 0.02388973\n",
      "Iteration 914, loss = 0.02387738\n",
      "Iteration 915, loss = 0.02386660\n",
      "Iteration 916, loss = 0.02385635\n",
      "Iteration 917, loss = 0.02384612\n",
      "Iteration 918, loss = 0.02383637\n",
      "Iteration 919, loss = 0.02382575\n",
      "Iteration 920, loss = 0.02381362\n",
      "Iteration 921, loss = 0.02380306\n",
      "Iteration 922, loss = 0.02379454\n",
      "Iteration 923, loss = 0.02378449\n",
      "Iteration 924, loss = 0.02377238\n",
      "Iteration 925, loss = 0.02375989\n",
      "Iteration 926, loss = 0.02375017\n",
      "Iteration 927, loss = 0.02373904\n",
      "Iteration 928, loss = 0.02372837\n",
      "Iteration 929, loss = 0.02371733\n",
      "Iteration 930, loss = 0.02370677\n",
      "Iteration 931, loss = 0.02369648\n",
      "Iteration 932, loss = 0.02368624\n",
      "Iteration 933, loss = 0.02367652\n",
      "Iteration 934, loss = 0.02366393\n",
      "Iteration 935, loss = 0.02365439\n",
      "Iteration 936, loss = 0.02364453\n",
      "Iteration 937, loss = 0.02363312\n",
      "Iteration 938, loss = 0.02362213\n",
      "Iteration 939, loss = 0.02361172\n",
      "Iteration 940, loss = 0.02360123\n",
      "Iteration 941, loss = 0.02359008\n",
      "Iteration 942, loss = 0.02357934\n",
      "Iteration 943, loss = 0.02357015\n",
      "Iteration 944, loss = 0.02355882\n",
      "Iteration 945, loss = 0.02354775\n",
      "Iteration 946, loss = 0.02353768\n",
      "Iteration 947, loss = 0.02352809\n",
      "Iteration 948, loss = 0.02351810\n",
      "Iteration 949, loss = 0.02350711\n",
      "Iteration 950, loss = 0.02349556\n",
      "Iteration 951, loss = 0.02348691\n",
      "Iteration 952, loss = 0.02347591\n",
      "Iteration 953, loss = 0.02346429\n",
      "Iteration 954, loss = 0.02345392\n",
      "Iteration 955, loss = 0.02344468\n",
      "Iteration 956, loss = 0.02343390\n",
      "Iteration 957, loss = 0.02342360\n",
      "Iteration 958, loss = 0.02341247\n",
      "Iteration 959, loss = 0.02340255\n",
      "Iteration 960, loss = 0.02339297\n",
      "Iteration 961, loss = 0.02338201\n",
      "Iteration 962, loss = 0.02337117\n",
      "Iteration 963, loss = 0.02336105\n",
      "Iteration 964, loss = 0.02335090\n",
      "Iteration 965, loss = 0.02334037\n",
      "Iteration 966, loss = 0.02333048\n",
      "Iteration 967, loss = 0.02331999\n",
      "Iteration 968, loss = 0.02330935\n",
      "Iteration 969, loss = 0.02330034\n",
      "Iteration 970, loss = 0.02329047\n",
      "Iteration 971, loss = 0.02327993\n",
      "Iteration 972, loss = 0.02326921\n",
      "Iteration 973, loss = 0.02325912\n",
      "Iteration 974, loss = 0.02324877\n",
      "Iteration 975, loss = 0.02323879\n",
      "Iteration 976, loss = 0.02322861\n",
      "Iteration 977, loss = 0.02321773\n",
      "Iteration 978, loss = 0.02320849\n",
      "Iteration 979, loss = 0.02319909\n",
      "Iteration 980, loss = 0.02318774\n",
      "Iteration 981, loss = 0.02317756\n",
      "Iteration 982, loss = 0.02316790\n",
      "Iteration 983, loss = 0.02315864\n",
      "Iteration 984, loss = 0.02314748\n",
      "Iteration 985, loss = 0.02313615\n",
      "Iteration 986, loss = 0.02312730\n",
      "Iteration 987, loss = 0.02311831\n",
      "Iteration 988, loss = 0.02310794\n",
      "Iteration 989, loss = 0.02309672\n",
      "Iteration 990, loss = 0.02308678\n",
      "Iteration 991, loss = 0.02307643\n",
      "Iteration 992, loss = 0.02306602\n",
      "Iteration 993, loss = 0.02305516\n",
      "Iteration 994, loss = 0.02304485\n",
      "Iteration 995, loss = 0.02303593\n",
      "Iteration 996, loss = 0.02302593\n",
      "Iteration 997, loss = 0.02301499\n",
      "Iteration 998, loss = 0.02300429\n",
      "Iteration 999, loss = 0.02299517\n",
      "Iteration 1000, loss = 0.02298561\n",
      "Iteration 1001, loss = 0.02297488\n",
      "Iteration 1002, loss = 0.02296489\n",
      "Iteration 1003, loss = 0.02295507\n",
      "Iteration 1004, loss = 0.02294409\n",
      "Iteration 1005, loss = 0.02293436\n",
      "Iteration 1006, loss = 0.02292488\n",
      "Iteration 1007, loss = 0.02291390\n",
      "Iteration 1008, loss = 0.02290410\n",
      "Iteration 1009, loss = 0.02289378\n",
      "Iteration 1010, loss = 0.02288408\n",
      "Iteration 1011, loss = 0.02287524\n",
      "Iteration 1012, loss = 0.02286447\n",
      "Iteration 1013, loss = 0.02285605\n",
      "Iteration 1014, loss = 0.02284730\n",
      "Iteration 1015, loss = 0.02283780\n",
      "Iteration 1016, loss = 0.02282764\n",
      "Iteration 1017, loss = 0.02281640\n",
      "Iteration 1018, loss = 0.02280623\n",
      "Iteration 1019, loss = 0.02279692\n",
      "Iteration 1020, loss = 0.02278790\n",
      "Iteration 1021, loss = 0.02277809\n",
      "Iteration 1022, loss = 0.02276822\n",
      "Iteration 1023, loss = 0.02275916\n",
      "Iteration 1024, loss = 0.02274971\n",
      "Iteration 1025, loss = 0.02274064\n",
      "Iteration 1026, loss = 0.02273046\n",
      "Iteration 1027, loss = 0.02271978\n",
      "Iteration 1028, loss = 0.02271081\n",
      "Iteration 1029, loss = 0.02270156\n",
      "Iteration 1030, loss = 0.02269089\n",
      "Iteration 1031, loss = 0.02268092\n",
      "Iteration 1032, loss = 0.02267270\n",
      "Iteration 1033, loss = 0.02266277\n",
      "Iteration 1034, loss = 0.02265252\n",
      "Iteration 1035, loss = 0.02264346\n",
      "Iteration 1036, loss = 0.02263454\n",
      "Iteration 1037, loss = 0.02262517\n",
      "Iteration 1038, loss = 0.02261606\n",
      "Iteration 1039, loss = 0.02260654\n",
      "Iteration 1040, loss = 0.02259646\n",
      "Iteration 1041, loss = 0.02258814\n",
      "Iteration 1042, loss = 0.02257846\n",
      "Iteration 1043, loss = 0.02256843\n",
      "Iteration 1044, loss = 0.02255898\n",
      "Iteration 1045, loss = 0.02255031\n",
      "Iteration 1046, loss = 0.02254062\n",
      "Iteration 1047, loss = 0.02253047\n",
      "Iteration 1048, loss = 0.02252173\n",
      "Iteration 1049, loss = 0.02251170\n",
      "Iteration 1050, loss = 0.02250351\n",
      "Iteration 1051, loss = 0.02249403\n",
      "Iteration 1052, loss = 0.02248401\n",
      "Iteration 1053, loss = 0.02247528\n",
      "Iteration 1054, loss = 0.02246571\n",
      "Iteration 1055, loss = 0.02245527\n",
      "Iteration 1056, loss = 0.02244670\n",
      "Iteration 1057, loss = 0.02243703\n",
      "Iteration 1058, loss = 0.02242867\n",
      "Iteration 1059, loss = 0.02241965\n",
      "Iteration 1060, loss = 0.02240749\n",
      "Iteration 1061, loss = 0.02239940\n",
      "Iteration 1062, loss = 0.02239130\n",
      "Iteration 1063, loss = 0.02238165\n",
      "Iteration 1064, loss = 0.02237170\n",
      "Iteration 1065, loss = 0.02236229\n",
      "Iteration 1066, loss = 0.02235387\n",
      "Iteration 1067, loss = 0.02234437\n",
      "Iteration 1068, loss = 0.02233521\n",
      "Iteration 1069, loss = 0.02232636\n",
      "Iteration 1070, loss = 0.02231678\n",
      "Iteration 1071, loss = 0.02230610\n",
      "Iteration 1072, loss = 0.02229694\n",
      "Iteration 1073, loss = 0.02228819\n",
      "Iteration 1074, loss = 0.02227910\n",
      "Iteration 1075, loss = 0.02227040\n",
      "Iteration 1076, loss = 0.02225995\n",
      "Iteration 1077, loss = 0.02225194\n",
      "Iteration 1078, loss = 0.02224288\n",
      "Iteration 1079, loss = 0.02223249\n",
      "Iteration 1080, loss = 0.02222309\n",
      "Iteration 1081, loss = 0.02221412\n",
      "Iteration 1082, loss = 0.02220465\n",
      "Iteration 1083, loss = 0.02219436\n",
      "Iteration 1084, loss = 0.02218621\n",
      "Iteration 1085, loss = 0.02217633\n",
      "Iteration 1086, loss = 0.02216630\n",
      "Iteration 1087, loss = 0.02215851\n",
      "Iteration 1088, loss = 0.02214817\n",
      "Iteration 1089, loss = 0.02213883\n",
      "Iteration 1090, loss = 0.02213016\n",
      "Iteration 1091, loss = 0.02212157\n",
      "Iteration 1092, loss = 0.02211181\n",
      "Iteration 1093, loss = 0.02210189\n",
      "Iteration 1094, loss = 0.02209363\n",
      "Iteration 1095, loss = 0.02208556\n",
      "Iteration 1096, loss = 0.02207510\n",
      "Iteration 1097, loss = 0.02206602\n",
      "Iteration 1098, loss = 0.02205867\n",
      "Iteration 1099, loss = 0.02205077\n",
      "Iteration 1100, loss = 0.02204016\n",
      "Iteration 1101, loss = 0.02203008\n",
      "Iteration 1102, loss = 0.02202103\n",
      "Iteration 1103, loss = 0.02201178\n",
      "Iteration 1104, loss = 0.02200327\n",
      "Iteration 1105, loss = 0.02199290\n",
      "Iteration 1106, loss = 0.02198286\n",
      "Iteration 1107, loss = 0.02197461\n",
      "Iteration 1108, loss = 0.02196744\n",
      "Iteration 1109, loss = 0.02195795\n",
      "Iteration 1110, loss = 0.02194682\n",
      "Iteration 1111, loss = 0.02193646\n",
      "Iteration 1112, loss = 0.02192746\n",
      "Iteration 1113, loss = 0.02191872\n",
      "Iteration 1114, loss = 0.02190935\n",
      "Iteration 1115, loss = 0.02190047\n",
      "Iteration 1116, loss = 0.02189204\n",
      "Iteration 1117, loss = 0.02188299\n",
      "Iteration 1118, loss = 0.02187270\n",
      "Iteration 1119, loss = 0.02186315\n",
      "Iteration 1120, loss = 0.02185499\n",
      "Iteration 1121, loss = 0.02184591\n",
      "Iteration 1122, loss = 0.02183562\n",
      "Iteration 1123, loss = 0.02182639\n",
      "Iteration 1124, loss = 0.02181806\n",
      "Iteration 1125, loss = 0.02180781\n",
      "Iteration 1126, loss = 0.02179942\n",
      "Iteration 1127, loss = 0.02179081\n",
      "Iteration 1128, loss = 0.02178148\n",
      "Iteration 1129, loss = 0.02177216\n",
      "Iteration 1130, loss = 0.02176353\n",
      "Iteration 1131, loss = 0.02175473\n",
      "Iteration 1132, loss = 0.02174450\n",
      "Iteration 1133, loss = 0.02173607\n",
      "Iteration 1134, loss = 0.02172645\n",
      "Iteration 1135, loss = 0.02171819\n",
      "Iteration 1136, loss = 0.02170999\n",
      "Iteration 1137, loss = 0.02170092\n",
      "Iteration 1138, loss = 0.02169064\n",
      "Iteration 1139, loss = 0.02168015\n",
      "Iteration 1140, loss = 0.02167206\n",
      "Iteration 1141, loss = 0.02166340\n",
      "Iteration 1142, loss = 0.02165338\n",
      "Iteration 1143, loss = 0.02164510\n",
      "Iteration 1144, loss = 0.02163578\n",
      "Iteration 1145, loss = 0.02162664\n",
      "Iteration 1146, loss = 0.02161764\n",
      "Iteration 1147, loss = 0.02160948\n",
      "Iteration 1148, loss = 0.02160036\n",
      "Iteration 1149, loss = 0.02159099\n",
      "Iteration 1150, loss = 0.02158033\n",
      "Iteration 1151, loss = 0.02157207\n",
      "Iteration 1152, loss = 0.02156310\n",
      "Iteration 1153, loss = 0.02155365\n",
      "Iteration 1154, loss = 0.02154520\n",
      "Iteration 1155, loss = 0.02153710\n",
      "Iteration 1156, loss = 0.02152779\n",
      "Iteration 1157, loss = 0.02151744\n",
      "Iteration 1158, loss = 0.02150815\n",
      "Iteration 1159, loss = 0.02149882\n",
      "Iteration 1160, loss = 0.02149057\n",
      "Iteration 1161, loss = 0.02148103\n",
      "Iteration 1162, loss = 0.02147108\n",
      "Iteration 1163, loss = 0.02146409\n",
      "Iteration 1164, loss = 0.02145534\n",
      "Iteration 1165, loss = 0.02144588\n",
      "Iteration 1166, loss = 0.02143624\n",
      "Iteration 1167, loss = 0.02142738\n",
      "Iteration 1168, loss = 0.02141927\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.08008595\n",
      "Iteration 2, loss = 0.07971518\n",
      "Iteration 3, loss = 0.07936117\n",
      "Iteration 4, loss = 0.07901286\n",
      "Iteration 5, loss = 0.07866731\n",
      "Iteration 6, loss = 0.07832429\n",
      "Iteration 7, loss = 0.07798328\n",
      "Iteration 8, loss = 0.07764403\n",
      "Iteration 9, loss = 0.07730172\n",
      "Iteration 10, loss = 0.07696067\n",
      "Iteration 11, loss = 0.07662073\n",
      "Iteration 12, loss = 0.07628183\n",
      "Iteration 13, loss = 0.07594513\n",
      "Iteration 14, loss = 0.07560965\n",
      "Iteration 15, loss = 0.07527550\n",
      "Iteration 16, loss = 0.07494502\n",
      "Iteration 17, loss = 0.07462318\n",
      "Iteration 18, loss = 0.07431421\n",
      "Iteration 19, loss = 0.07400793\n",
      "Iteration 20, loss = 0.07370508\n",
      "Iteration 21, loss = 0.07340445\n",
      "Iteration 22, loss = 0.07310460\n",
      "Iteration 23, loss = 0.07279937\n",
      "Iteration 24, loss = 0.07249466\n",
      "Iteration 25, loss = 0.07218935\n",
      "Iteration 26, loss = 0.07188536\n",
      "Iteration 27, loss = 0.07158500\n",
      "Iteration 28, loss = 0.07129585\n",
      "Iteration 29, loss = 0.07100773\n",
      "Iteration 30, loss = 0.07072077\n",
      "Iteration 31, loss = 0.07043523\n",
      "Iteration 32, loss = 0.07015099\n",
      "Iteration 33, loss = 0.06986932\n",
      "Iteration 34, loss = 0.06959123\n",
      "Iteration 35, loss = 0.06931451\n",
      "Iteration 36, loss = 0.06903947\n",
      "Iteration 37, loss = 0.06877585\n",
      "Iteration 38, loss = 0.06851326\n",
      "Iteration 39, loss = 0.06825180\n",
      "Iteration 40, loss = 0.06799152\n",
      "Iteration 41, loss = 0.06773394\n",
      "Iteration 42, loss = 0.06747744\n",
      "Iteration 43, loss = 0.06722180\n",
      "Iteration 44, loss = 0.06696710\n",
      "Iteration 45, loss = 0.06671336\n",
      "Iteration 46, loss = 0.06645998\n",
      "Iteration 47, loss = 0.06620762\n",
      "Iteration 48, loss = 0.06595638\n",
      "Iteration 49, loss = 0.06570678\n",
      "Iteration 50, loss = 0.06545920\n",
      "Iteration 51, loss = 0.06521281\n",
      "Iteration 52, loss = 0.06496764\n",
      "Iteration 53, loss = 0.06472366\n",
      "Iteration 54, loss = 0.06448096\n",
      "Iteration 55, loss = 0.06423945\n",
      "Iteration 56, loss = 0.06399912\n",
      "Iteration 57, loss = 0.06375996\n",
      "Iteration 58, loss = 0.06352208\n",
      "Iteration 59, loss = 0.06328540\n",
      "Iteration 60, loss = 0.06305222\n",
      "Iteration 61, loss = 0.06281884\n",
      "Iteration 62, loss = 0.06258751\n",
      "Iteration 63, loss = 0.06235820\n",
      "Iteration 64, loss = 0.06212953\n",
      "Iteration 65, loss = 0.06190169\n",
      "Iteration 66, loss = 0.06167457\n",
      "Iteration 67, loss = 0.06144781\n",
      "Iteration 68, loss = 0.06122489\n",
      "Iteration 69, loss = 0.06099912\n",
      "Iteration 70, loss = 0.06077596\n",
      "Iteration 71, loss = 0.06055882\n",
      "Iteration 72, loss = 0.06034527\n",
      "Iteration 73, loss = 0.06013265\n",
      "Iteration 74, loss = 0.05992151\n",
      "Iteration 75, loss = 0.05971130\n",
      "Iteration 76, loss = 0.05950187\n",
      "Iteration 77, loss = 0.05929330\n",
      "Iteration 78, loss = 0.05908591\n",
      "Iteration 79, loss = 0.05887893\n",
      "Iteration 80, loss = 0.05867004\n",
      "Iteration 81, loss = 0.05846054\n",
      "Iteration 82, loss = 0.05825117\n",
      "Iteration 83, loss = 0.05804210\n",
      "Iteration 84, loss = 0.05783352\n",
      "Iteration 85, loss = 0.05762500\n",
      "Iteration 86, loss = 0.05741599\n",
      "Iteration 87, loss = 0.05720749\n",
      "Iteration 88, loss = 0.05700008\n",
      "Iteration 89, loss = 0.05679307\n",
      "Iteration 90, loss = 0.05659006\n",
      "Iteration 91, loss = 0.05638973\n",
      "Iteration 92, loss = 0.05618959\n",
      "Iteration 93, loss = 0.05598990\n",
      "Iteration 94, loss = 0.05579128\n",
      "Iteration 95, loss = 0.05559243\n",
      "Iteration 96, loss = 0.05539472\n",
      "Iteration 97, loss = 0.05519761\n",
      "Iteration 98, loss = 0.05500121\n",
      "Iteration 99, loss = 0.05480545\n",
      "Iteration 100, loss = 0.05461068\n",
      "Iteration 101, loss = 0.05441964\n",
      "Iteration 102, loss = 0.05422918\n",
      "Iteration 103, loss = 0.05403928\n",
      "Iteration 104, loss = 0.05384430\n",
      "Iteration 105, loss = 0.05364932\n",
      "Iteration 106, loss = 0.05345677\n",
      "Iteration 107, loss = 0.05326103\n",
      "Iteration 108, loss = 0.05306909\n",
      "Iteration 109, loss = 0.05287817\n",
      "Iteration 110, loss = 0.05268731\n",
      "Iteration 111, loss = 0.05249671\n",
      "Iteration 112, loss = 0.05230540\n",
      "Iteration 113, loss = 0.05211516\n",
      "Iteration 114, loss = 0.05192662\n",
      "Iteration 115, loss = 0.05173948\n",
      "Iteration 116, loss = 0.05155183\n",
      "Iteration 117, loss = 0.05136400\n",
      "Iteration 118, loss = 0.05117643\n",
      "Iteration 119, loss = 0.05098645\n",
      "Iteration 120, loss = 0.05079757\n",
      "Iteration 121, loss = 0.05061078\n",
      "Iteration 122, loss = 0.05042620\n",
      "Iteration 123, loss = 0.05024066\n",
      "Iteration 124, loss = 0.05006012\n",
      "Iteration 125, loss = 0.04988404\n",
      "Iteration 126, loss = 0.04971100\n",
      "Iteration 127, loss = 0.04953807\n",
      "Iteration 128, loss = 0.04936632\n",
      "Iteration 129, loss = 0.04919492\n",
      "Iteration 130, loss = 0.04902770\n",
      "Iteration 131, loss = 0.04885990\n",
      "Iteration 132, loss = 0.04869173\n",
      "Iteration 133, loss = 0.04852508\n",
      "Iteration 134, loss = 0.04836091\n",
      "Iteration 135, loss = 0.04819761\n",
      "Iteration 136, loss = 0.04803541\n",
      "Iteration 137, loss = 0.04787372\n",
      "Iteration 138, loss = 0.04771363\n",
      "Iteration 139, loss = 0.04755591\n",
      "Iteration 140, loss = 0.04739878\n",
      "Iteration 141, loss = 0.04724397\n",
      "Iteration 142, loss = 0.04709046\n",
      "Iteration 143, loss = 0.04693748\n",
      "Iteration 144, loss = 0.04678593\n",
      "Iteration 145, loss = 0.04663583\n",
      "Iteration 146, loss = 0.04648685\n",
      "Iteration 147, loss = 0.04633935\n",
      "Iteration 148, loss = 0.04619306\n",
      "Iteration 149, loss = 0.04604745\n",
      "Iteration 150, loss = 0.04590345\n",
      "Iteration 151, loss = 0.04576027\n",
      "Iteration 152, loss = 0.04561851\n",
      "Iteration 153, loss = 0.04547990\n",
      "Iteration 154, loss = 0.04534076\n",
      "Iteration 155, loss = 0.04520201\n",
      "Iteration 156, loss = 0.04506421\n",
      "Iteration 157, loss = 0.04492893\n",
      "Iteration 158, loss = 0.04479350\n",
      "Iteration 159, loss = 0.04466021\n",
      "Iteration 160, loss = 0.04452704\n",
      "Iteration 161, loss = 0.04439428\n",
      "Iteration 162, loss = 0.04426257\n",
      "Iteration 163, loss = 0.04413316\n",
      "Iteration 164, loss = 0.04400426\n",
      "Iteration 165, loss = 0.04387627\n",
      "Iteration 166, loss = 0.04374960\n",
      "Iteration 167, loss = 0.04362381\n",
      "Iteration 168, loss = 0.04350007\n",
      "Iteration 169, loss = 0.04337785\n",
      "Iteration 170, loss = 0.04325618\n",
      "Iteration 171, loss = 0.04313541\n",
      "Iteration 172, loss = 0.04301541\n",
      "Iteration 173, loss = 0.04289469\n",
      "Iteration 174, loss = 0.04277659\n",
      "Iteration 175, loss = 0.04266087\n",
      "Iteration 176, loss = 0.04254475\n",
      "Iteration 177, loss = 0.04243062\n",
      "Iteration 178, loss = 0.04231761\n",
      "Iteration 179, loss = 0.04220398\n",
      "Iteration 180, loss = 0.04209218\n",
      "Iteration 181, loss = 0.04198213\n",
      "Iteration 182, loss = 0.04187154\n",
      "Iteration 183, loss = 0.04176318\n",
      "Iteration 184, loss = 0.04165588\n",
      "Iteration 185, loss = 0.04154970\n",
      "Iteration 186, loss = 0.04144215\n",
      "Iteration 187, loss = 0.04133362\n",
      "Iteration 188, loss = 0.04122586\n",
      "Iteration 189, loss = 0.04111810\n",
      "Iteration 190, loss = 0.04101057\n",
      "Iteration 191, loss = 0.04090400\n",
      "Iteration 192, loss = 0.04079763\n",
      "Iteration 193, loss = 0.04069274\n",
      "Iteration 194, loss = 0.04058900\n",
      "Iteration 195, loss = 0.04048517\n",
      "Iteration 196, loss = 0.04038232\n",
      "Iteration 197, loss = 0.04027996\n",
      "Iteration 198, loss = 0.04017932\n",
      "Iteration 199, loss = 0.04007955\n",
      "Iteration 200, loss = 0.03998024\n",
      "Iteration 201, loss = 0.03988174\n",
      "Iteration 202, loss = 0.03978428\n",
      "Iteration 203, loss = 0.03968819\n",
      "Iteration 204, loss = 0.03959288\n",
      "Iteration 205, loss = 0.03949823\n",
      "Iteration 206, loss = 0.03940475\n",
      "Iteration 207, loss = 0.03931220\n",
      "Iteration 208, loss = 0.03922029\n",
      "Iteration 209, loss = 0.03912936\n",
      "Iteration 210, loss = 0.03903949\n",
      "Iteration 211, loss = 0.03895036\n",
      "Iteration 212, loss = 0.03886219\n",
      "Iteration 213, loss = 0.03877491\n",
      "Iteration 214, loss = 0.03868851\n",
      "Iteration 215, loss = 0.03860290\n",
      "Iteration 216, loss = 0.03851836\n",
      "Iteration 217, loss = 0.03843466\n",
      "Iteration 218, loss = 0.03835247\n",
      "Iteration 219, loss = 0.03827153\n",
      "Iteration 220, loss = 0.03819301\n",
      "Iteration 221, loss = 0.03811529\n",
      "Iteration 222, loss = 0.03803928\n",
      "Iteration 223, loss = 0.03796362\n",
      "Iteration 224, loss = 0.03788851\n",
      "Iteration 225, loss = 0.03781391\n",
      "Iteration 226, loss = 0.03774022\n",
      "Iteration 227, loss = 0.03766737\n",
      "Iteration 228, loss = 0.03759481\n",
      "Iteration 229, loss = 0.03752278\n",
      "Iteration 230, loss = 0.03745224\n",
      "Iteration 231, loss = 0.03738243\n",
      "Iteration 232, loss = 0.03731310\n",
      "Iteration 233, loss = 0.03724464\n",
      "Iteration 234, loss = 0.03717776\n",
      "Iteration 235, loss = 0.03711105\n",
      "Iteration 236, loss = 0.03704516\n",
      "Iteration 237, loss = 0.03698012\n",
      "Iteration 238, loss = 0.03691556\n",
      "Iteration 239, loss = 0.03685167\n",
      "Iteration 240, loss = 0.03678847\n",
      "Iteration 241, loss = 0.03672614\n",
      "Iteration 242, loss = 0.03666469\n",
      "Iteration 243, loss = 0.03660395\n",
      "Iteration 244, loss = 0.03654351\n",
      "Iteration 245, loss = 0.03648374\n",
      "Iteration 246, loss = 0.03642465\n",
      "Iteration 247, loss = 0.03636635\n",
      "Iteration 248, loss = 0.03630904\n",
      "Iteration 249, loss = 0.03625232\n",
      "Iteration 250, loss = 0.03619645\n",
      "Iteration 251, loss = 0.03614095\n",
      "Iteration 252, loss = 0.03608614\n",
      "Iteration 253, loss = 0.03603221\n",
      "Iteration 254, loss = 0.03597900\n",
      "Iteration 255, loss = 0.03592622\n",
      "Iteration 256, loss = 0.03587389\n",
      "Iteration 257, loss = 0.03582235\n",
      "Iteration 258, loss = 0.03577145\n",
      "Iteration 259, loss = 0.03572095\n",
      "Iteration 260, loss = 0.03567114\n",
      "Iteration 261, loss = 0.03562177\n",
      "Iteration 262, loss = 0.03557331\n",
      "Iteration 263, loss = 0.03552525\n",
      "Iteration 264, loss = 0.03547782\n",
      "Iteration 265, loss = 0.03543094\n",
      "Iteration 266, loss = 0.03538449\n",
      "Iteration 267, loss = 0.03533859\n",
      "Iteration 268, loss = 0.03529321\n",
      "Iteration 269, loss = 0.03524826\n",
      "Iteration 270, loss = 0.03520414\n",
      "Iteration 271, loss = 0.03516036\n",
      "Iteration 272, loss = 0.03511677\n",
      "Iteration 273, loss = 0.03507396\n",
      "Iteration 274, loss = 0.03503160\n",
      "Iteration 275, loss = 0.03498962\n",
      "Iteration 276, loss = 0.03494821\n",
      "Iteration 277, loss = 0.03490713\n",
      "Iteration 278, loss = 0.03486671\n",
      "Iteration 279, loss = 0.03482655\n",
      "Iteration 280, loss = 0.03478694\n",
      "Iteration 281, loss = 0.03474768\n",
      "Iteration 282, loss = 0.03470851\n",
      "Iteration 283, loss = 0.03466915\n",
      "Iteration 284, loss = 0.03462983\n",
      "Iteration 285, loss = 0.03459040\n",
      "Iteration 286, loss = 0.03455123\n",
      "Iteration 287, loss = 0.03451229\n",
      "Iteration 288, loss = 0.03447314\n",
      "Iteration 289, loss = 0.03443433\n",
      "Iteration 290, loss = 0.03439574\n",
      "Iteration 291, loss = 0.03435731\n",
      "Iteration 292, loss = 0.03431905\n",
      "Iteration 293, loss = 0.03428100\n",
      "Iteration 294, loss = 0.03424335\n",
      "Iteration 295, loss = 0.03420619\n",
      "Iteration 296, loss = 0.03416897\n",
      "Iteration 297, loss = 0.03413182\n",
      "Iteration 298, loss = 0.03409505\n",
      "Iteration 299, loss = 0.03405863\n",
      "Iteration 300, loss = 0.03402248\n",
      "Iteration 301, loss = 0.03398654\n",
      "Iteration 302, loss = 0.03395459\n",
      "Iteration 303, loss = 0.03392279\n",
      "Iteration 304, loss = 0.03389116\n",
      "Iteration 305, loss = 0.03386021\n",
      "Iteration 306, loss = 0.03382871\n",
      "Iteration 307, loss = 0.03379801\n",
      "Iteration 308, loss = 0.03376757\n",
      "Iteration 309, loss = 0.03373709\n",
      "Iteration 310, loss = 0.03370692\n",
      "Iteration 311, loss = 0.03367715\n",
      "Iteration 312, loss = 0.03364751\n",
      "Iteration 313, loss = 0.03361808\n",
      "Iteration 314, loss = 0.03358884\n",
      "Iteration 315, loss = 0.03355976\n",
      "Iteration 316, loss = 0.03353084\n",
      "Iteration 317, loss = 0.03350210\n",
      "Iteration 318, loss = 0.03347396\n",
      "Iteration 319, loss = 0.03344586\n",
      "Iteration 320, loss = 0.03341781\n",
      "Iteration 321, loss = 0.03339012\n",
      "Iteration 322, loss = 0.03336327\n",
      "Iteration 323, loss = 0.03333660\n",
      "Iteration 324, loss = 0.03330987\n",
      "Iteration 325, loss = 0.03328312\n",
      "Iteration 326, loss = 0.03325635\n",
      "Iteration 327, loss = 0.03322961\n",
      "Iteration 328, loss = 0.03320317\n",
      "Iteration 329, loss = 0.03317733\n",
      "Iteration 330, loss = 0.03315245\n",
      "Iteration 331, loss = 0.03312742\n",
      "Iteration 332, loss = 0.03310248\n",
      "Iteration 333, loss = 0.03307852\n",
      "Iteration 334, loss = 0.03305368\n",
      "Iteration 335, loss = 0.03302930\n",
      "Iteration 336, loss = 0.03300506\n",
      "Iteration 337, loss = 0.03298083\n",
      "Iteration 338, loss = 0.03295670\n",
      "Iteration 339, loss = 0.03293274\n",
      "Iteration 340, loss = 0.03290901\n",
      "Iteration 341, loss = 0.03288534\n",
      "Iteration 342, loss = 0.03286142\n",
      "Iteration 343, loss = 0.03283691\n",
      "Iteration 344, loss = 0.03281199\n",
      "Iteration 345, loss = 0.03278727\n",
      "Iteration 346, loss = 0.03276553\n",
      "Iteration 347, loss = 0.03274360\n",
      "Iteration 348, loss = 0.03272158\n",
      "Iteration 349, loss = 0.03269946\n",
      "Iteration 350, loss = 0.03267724\n",
      "Iteration 351, loss = 0.03265500\n",
      "Iteration 352, loss = 0.03263283\n",
      "Iteration 353, loss = 0.03261074\n",
      "Iteration 354, loss = 0.03258856\n",
      "Iteration 355, loss = 0.03256639\n",
      "Iteration 356, loss = 0.03254411\n",
      "Iteration 357, loss = 0.03252184\n",
      "Iteration 358, loss = 0.03249978\n",
      "Iteration 359, loss = 0.03247727\n",
      "Iteration 360, loss = 0.03245451\n",
      "Iteration 361, loss = 0.03243179\n",
      "Iteration 362, loss = 0.03240912\n",
      "Iteration 363, loss = 0.03238635\n",
      "Iteration 364, loss = 0.03236375\n",
      "Iteration 365, loss = 0.03234055\n",
      "Iteration 366, loss = 0.03231786\n",
      "Iteration 367, loss = 0.03229512\n",
      "Iteration 368, loss = 0.03227239\n",
      "Iteration 369, loss = 0.03224941\n",
      "Iteration 370, loss = 0.03222672\n",
      "Iteration 371, loss = 0.03220397\n",
      "Iteration 372, loss = 0.03218128\n",
      "Iteration 373, loss = 0.03215863\n",
      "Iteration 374, loss = 0.03213597\n",
      "Iteration 375, loss = 0.03211680\n",
      "Iteration 376, loss = 0.03209784\n",
      "Iteration 377, loss = 0.03207899\n",
      "Iteration 378, loss = 0.03206011\n",
      "Iteration 379, loss = 0.03204130\n",
      "Iteration 380, loss = 0.03202260\n",
      "Iteration 381, loss = 0.03200389\n",
      "Iteration 382, loss = 0.03198536\n",
      "Iteration 383, loss = 0.03196688\n",
      "Iteration 384, loss = 0.03194844\n",
      "Iteration 385, loss = 0.03193001\n",
      "Iteration 386, loss = 0.03191166\n",
      "Iteration 387, loss = 0.03189340\n",
      "Iteration 388, loss = 0.03187527\n",
      "Iteration 389, loss = 0.03185722\n",
      "Iteration 390, loss = 0.03183890\n",
      "Iteration 391, loss = 0.03182099\n",
      "Iteration 392, loss = 0.03180291\n",
      "Iteration 393, loss = 0.03178489\n",
      "Iteration 394, loss = 0.03176699\n",
      "Iteration 395, loss = 0.03174967\n",
      "Iteration 396, loss = 0.03173404\n",
      "Iteration 397, loss = 0.03171827\n",
      "Iteration 398, loss = 0.03170268\n",
      "Iteration 399, loss = 0.03168705\n",
      "Iteration 400, loss = 0.03167138\n",
      "Iteration 401, loss = 0.03165579\n",
      "Iteration 402, loss = 0.03164026\n",
      "Iteration 403, loss = 0.03162462\n",
      "Iteration 404, loss = 0.03160903\n",
      "Iteration 405, loss = 0.03159342\n",
      "Iteration 406, loss = 0.03157774\n",
      "Iteration 407, loss = 0.03156213\n",
      "Iteration 408, loss = 0.03154652\n",
      "Iteration 409, loss = 0.03153107\n",
      "Iteration 410, loss = 0.03151567\n",
      "Iteration 411, loss = 0.03149855\n",
      "Iteration 412, loss = 0.03148059\n",
      "Iteration 413, loss = 0.03146235\n",
      "Iteration 414, loss = 0.03144355\n",
      "Iteration 415, loss = 0.03142422\n",
      "Iteration 416, loss = 0.03140445\n",
      "Iteration 417, loss = 0.03138463\n",
      "Iteration 418, loss = 0.03136424\n",
      "Iteration 419, loss = 0.03134347\n",
      "Iteration 420, loss = 0.03132301\n",
      "Iteration 421, loss = 0.03130191\n",
      "Iteration 422, loss = 0.03128070\n",
      "Iteration 423, loss = 0.03125937\n",
      "Iteration 424, loss = 0.03123786\n",
      "Iteration 425, loss = 0.03121625\n",
      "Iteration 426, loss = 0.03119456\n",
      "Iteration 427, loss = 0.03117283\n",
      "Iteration 428, loss = 0.03115151\n",
      "Iteration 429, loss = 0.03112937\n",
      "Iteration 430, loss = 0.03110760\n",
      "Iteration 431, loss = 0.03108572\n",
      "Iteration 432, loss = 0.03106380\n",
      "Iteration 433, loss = 0.03104218\n",
      "Iteration 434, loss = 0.03102023\n",
      "Iteration 435, loss = 0.03099814\n",
      "Iteration 436, loss = 0.03097631\n",
      "Iteration 437, loss = 0.03095432\n",
      "Iteration 438, loss = 0.03093226\n",
      "Iteration 439, loss = 0.03091000\n",
      "Iteration 440, loss = 0.03088780\n",
      "Iteration 441, loss = 0.03086548\n",
      "Iteration 442, loss = 0.03084361\n",
      "Iteration 443, loss = 0.03082112\n",
      "Iteration 444, loss = 0.03079904\n",
      "Iteration 445, loss = 0.03077688\n",
      "Iteration 446, loss = 0.03075477\n",
      "Iteration 447, loss = 0.03073267\n",
      "Iteration 448, loss = 0.03071052\n",
      "Iteration 449, loss = 0.03068828\n",
      "Iteration 450, loss = 0.03066621\n",
      "Iteration 451, loss = 0.03064356\n",
      "Iteration 452, loss = 0.03062035\n",
      "Iteration 453, loss = 0.03059699\n",
      "Iteration 454, loss = 0.03057335\n",
      "Iteration 455, loss = 0.03054970\n",
      "Iteration 456, loss = 0.03052568\n",
      "Iteration 457, loss = 0.03050173\n",
      "Iteration 458, loss = 0.03047777\n",
      "Iteration 459, loss = 0.03045366\n",
      "Iteration 460, loss = 0.03042962\n",
      "Iteration 461, loss = 0.03040590\n",
      "Iteration 462, loss = 0.03038132\n",
      "Iteration 463, loss = 0.03035729\n",
      "Iteration 464, loss = 0.03033316\n",
      "Iteration 465, loss = 0.03030885\n",
      "Iteration 466, loss = 0.03028635\n",
      "Iteration 467, loss = 0.03026549\n",
      "Iteration 468, loss = 0.03024491\n",
      "Iteration 469, loss = 0.03022450\n",
      "Iteration 470, loss = 0.03020431\n",
      "Iteration 471, loss = 0.03018415\n",
      "Iteration 472, loss = 0.03016411\n",
      "Iteration 473, loss = 0.03014411\n",
      "Iteration 474, loss = 0.03012748\n",
      "Iteration 475, loss = 0.03011153\n",
      "Iteration 476, loss = 0.03009326\n",
      "Iteration 477, loss = 0.03007231\n",
      "Iteration 478, loss = 0.03005289\n",
      "Iteration 479, loss = 0.03003746\n",
      "Iteration 480, loss = 0.03002168\n",
      "Iteration 481, loss = 0.03000407\n",
      "Iteration 482, loss = 0.02998563\n",
      "Iteration 483, loss = 0.02996856\n",
      "Iteration 484, loss = 0.02995121\n",
      "Iteration 485, loss = 0.02993364\n",
      "Iteration 486, loss = 0.02991613\n",
      "Iteration 487, loss = 0.02989851\n",
      "Iteration 488, loss = 0.02988060\n",
      "Iteration 489, loss = 0.02986287\n",
      "Iteration 490, loss = 0.02984680\n",
      "Iteration 491, loss = 0.02983033\n",
      "Iteration 492, loss = 0.02981170\n",
      "Iteration 493, loss = 0.02979581\n",
      "Iteration 494, loss = 0.02977871\n",
      "Iteration 495, loss = 0.02976128\n",
      "Iteration 496, loss = 0.02974544\n",
      "Iteration 497, loss = 0.02972994\n",
      "Iteration 498, loss = 0.02971535\n",
      "Iteration 499, loss = 0.02969924\n",
      "Iteration 500, loss = 0.02968516\n",
      "Iteration 501, loss = 0.02967035\n",
      "Iteration 502, loss = 0.02965515\n",
      "Iteration 503, loss = 0.02964012\n",
      "Iteration 504, loss = 0.02962473\n",
      "Iteration 505, loss = 0.02961104\n",
      "Iteration 506, loss = 0.02959542\n",
      "Iteration 507, loss = 0.02957893\n",
      "Iteration 508, loss = 0.02956367\n",
      "Iteration 509, loss = 0.02954894\n",
      "Iteration 510, loss = 0.02953419\n",
      "Iteration 511, loss = 0.02951927\n",
      "Iteration 512, loss = 0.02950479\n",
      "Iteration 513, loss = 0.02949013\n",
      "Iteration 514, loss = 0.02947544\n",
      "Iteration 515, loss = 0.02946048\n",
      "Iteration 516, loss = 0.02944550\n",
      "Iteration 517, loss = 0.02943022\n",
      "Iteration 518, loss = 0.02941507\n",
      "Iteration 519, loss = 0.02940176\n",
      "Iteration 520, loss = 0.02938682\n",
      "Iteration 521, loss = 0.02937115\n",
      "Iteration 522, loss = 0.02935696\n",
      "Iteration 523, loss = 0.02934252\n",
      "Iteration 524, loss = 0.02932770\n",
      "Iteration 525, loss = 0.02931297\n",
      "Iteration 526, loss = 0.02929898\n",
      "Iteration 527, loss = 0.02928355\n",
      "Iteration 528, loss = 0.02926903\n",
      "Iteration 529, loss = 0.02925475\n",
      "Iteration 530, loss = 0.02924034\n",
      "Iteration 531, loss = 0.02922610\n",
      "Iteration 532, loss = 0.02921173\n",
      "Iteration 533, loss = 0.02919728\n",
      "Iteration 534, loss = 0.02918316\n",
      "Iteration 535, loss = 0.02917175\n",
      "Iteration 536, loss = 0.02916026\n",
      "Iteration 537, loss = 0.02914710\n",
      "Iteration 538, loss = 0.02913508\n",
      "Iteration 539, loss = 0.02912203\n",
      "Iteration 540, loss = 0.02910794\n",
      "Iteration 541, loss = 0.02909431\n",
      "Iteration 542, loss = 0.02907974\n",
      "Iteration 543, loss = 0.02906727\n",
      "Iteration 544, loss = 0.02905652\n",
      "Iteration 545, loss = 0.02904372\n",
      "Iteration 546, loss = 0.02903003\n",
      "Iteration 547, loss = 0.02901847\n",
      "Iteration 548, loss = 0.02900518\n",
      "Iteration 549, loss = 0.02899216\n",
      "Iteration 550, loss = 0.02897964\n",
      "Iteration 551, loss = 0.02896829\n",
      "Iteration 552, loss = 0.02895590\n",
      "Iteration 553, loss = 0.02894454\n",
      "Iteration 554, loss = 0.02893163\n",
      "Iteration 555, loss = 0.02891721\n",
      "Iteration 556, loss = 0.02890557\n",
      "Iteration 557, loss = 0.02889355\n",
      "Iteration 558, loss = 0.02888296\n",
      "Iteration 559, loss = 0.02887113\n",
      "Iteration 560, loss = 0.02885756\n",
      "Iteration 561, loss = 0.02884536\n",
      "Iteration 562, loss = 0.02883319\n",
      "Iteration 563, loss = 0.02882110\n",
      "Iteration 564, loss = 0.02880761\n",
      "Iteration 565, loss = 0.02879673\n",
      "Iteration 566, loss = 0.02878551\n",
      "Iteration 567, loss = 0.02877253\n",
      "Iteration 568, loss = 0.02875940\n",
      "Iteration 569, loss = 0.02874736\n",
      "Iteration 570, loss = 0.02873535\n",
      "Iteration 571, loss = 0.02872268\n",
      "Iteration 572, loss = 0.02871067\n",
      "Iteration 573, loss = 0.02869903\n",
      "Iteration 574, loss = 0.02868545\n",
      "Iteration 575, loss = 0.02867407\n",
      "Iteration 576, loss = 0.02866186\n",
      "Iteration 577, loss = 0.02864903\n",
      "Iteration 578, loss = 0.02863726\n",
      "Iteration 579, loss = 0.02862593\n",
      "Iteration 580, loss = 0.02861315\n",
      "Iteration 581, loss = 0.02860136\n",
      "Iteration 582, loss = 0.02858951\n",
      "Iteration 583, loss = 0.02857746\n",
      "Iteration 584, loss = 0.02856604\n",
      "Iteration 585, loss = 0.02855456\n",
      "Iteration 586, loss = 0.02854276\n",
      "Iteration 587, loss = 0.02852973\n",
      "Iteration 588, loss = 0.02851769\n",
      "Iteration 589, loss = 0.02850661\n",
      "Iteration 590, loss = 0.02849490\n",
      "Iteration 591, loss = 0.02848245\n",
      "Iteration 592, loss = 0.02847072\n",
      "Iteration 593, loss = 0.02845915\n",
      "Iteration 594, loss = 0.02844712\n",
      "Iteration 595, loss = 0.02843555\n",
      "Iteration 596, loss = 0.02842554\n",
      "Iteration 597, loss = 0.02841305\n",
      "Iteration 598, loss = 0.02839980\n",
      "Iteration 599, loss = 0.02838983\n",
      "Iteration 600, loss = 0.02837935\n",
      "Iteration 601, loss = 0.02836841\n",
      "Iteration 602, loss = 0.02835654\n",
      "Iteration 603, loss = 0.02834464\n",
      "Iteration 604, loss = 0.02833246\n",
      "Iteration 605, loss = 0.02832011\n",
      "Iteration 606, loss = 0.02830786\n",
      "Iteration 607, loss = 0.02829728\n",
      "Iteration 608, loss = 0.02828622\n",
      "Iteration 609, loss = 0.02827371\n",
      "Iteration 610, loss = 0.02826190\n",
      "Iteration 611, loss = 0.02825063\n",
      "Iteration 612, loss = 0.02823941\n",
      "Iteration 613, loss = 0.02822857\n",
      "Iteration 614, loss = 0.02821622\n",
      "Iteration 615, loss = 0.02820447\n",
      "Iteration 616, loss = 0.02819533\n",
      "Iteration 617, loss = 0.02818523\n",
      "Iteration 618, loss = 0.02817395\n",
      "Iteration 619, loss = 0.02816336\n",
      "Iteration 620, loss = 0.02815363\n",
      "Iteration 621, loss = 0.02814369\n",
      "Iteration 622, loss = 0.02813287\n",
      "Iteration 623, loss = 0.02812289\n",
      "Iteration 624, loss = 0.02811172\n",
      "Iteration 625, loss = 0.02810311\n",
      "Iteration 626, loss = 0.02809238\n",
      "Iteration 627, loss = 0.02808186\n",
      "Iteration 628, loss = 0.02807194\n",
      "Iteration 629, loss = 0.02806199\n",
      "Iteration 630, loss = 0.02805173\n",
      "Iteration 631, loss = 0.02804173\n",
      "Iteration 632, loss = 0.02803163\n",
      "Iteration 633, loss = 0.02802087\n",
      "Iteration 634, loss = 0.02801064\n",
      "Iteration 635, loss = 0.02800071\n",
      "Iteration 636, loss = 0.02799068\n",
      "Iteration 637, loss = 0.02798077\n",
      "Iteration 638, loss = 0.02797021\n",
      "Iteration 639, loss = 0.02795914\n",
      "Iteration 640, loss = 0.02794864\n",
      "Iteration 641, loss = 0.02793887\n",
      "Iteration 642, loss = 0.02792829\n",
      "Iteration 643, loss = 0.02791776\n",
      "Iteration 644, loss = 0.02790827\n",
      "Iteration 645, loss = 0.02789890\n",
      "Iteration 646, loss = 0.02788824\n",
      "Iteration 647, loss = 0.02787751\n",
      "Iteration 648, loss = 0.02786804\n",
      "Iteration 649, loss = 0.02785776\n",
      "Iteration 650, loss = 0.02784808\n",
      "Iteration 651, loss = 0.02783685\n",
      "Iteration 652, loss = 0.02782792\n",
      "Iteration 653, loss = 0.02781833\n",
      "Iteration 654, loss = 0.02780742\n",
      "Iteration 655, loss = 0.02779833\n",
      "Iteration 656, loss = 0.02778877\n",
      "Iteration 657, loss = 0.02778015\n",
      "Iteration 658, loss = 0.02776971\n",
      "Iteration 659, loss = 0.02775913\n",
      "Iteration 660, loss = 0.02774834\n",
      "Iteration 661, loss = 0.02773834\n",
      "Iteration 662, loss = 0.02772829\n",
      "Iteration 663, loss = 0.02771833\n",
      "Iteration 664, loss = 0.02770808\n",
      "Iteration 665, loss = 0.02769905\n",
      "Iteration 666, loss = 0.02768822\n",
      "Iteration 667, loss = 0.02767969\n",
      "Iteration 668, loss = 0.02766959\n",
      "Iteration 669, loss = 0.02765882\n",
      "Iteration 670, loss = 0.02764853\n",
      "Iteration 671, loss = 0.02763801\n",
      "Iteration 672, loss = 0.02762786\n",
      "Iteration 673, loss = 0.02761753\n",
      "Iteration 674, loss = 0.02760836\n",
      "Iteration 675, loss = 0.02759843\n",
      "Iteration 676, loss = 0.02758763\n",
      "Iteration 677, loss = 0.02757867\n",
      "Iteration 678, loss = 0.02756861\n",
      "Iteration 679, loss = 0.02755955\n",
      "Iteration 680, loss = 0.02754918\n",
      "Iteration 681, loss = 0.02753961\n",
      "Iteration 682, loss = 0.02752928\n",
      "Iteration 683, loss = 0.02751874\n",
      "Iteration 684, loss = 0.02751043\n",
      "Iteration 685, loss = 0.02750071\n",
      "Iteration 686, loss = 0.02749019\n",
      "Iteration 687, loss = 0.02747995\n",
      "Iteration 688, loss = 0.02747077\n",
      "Iteration 689, loss = 0.02746155\n",
      "Iteration 690, loss = 0.02745287\n",
      "Iteration 691, loss = 0.02744402\n",
      "Iteration 692, loss = 0.02743426\n",
      "Iteration 693, loss = 0.02742349\n",
      "Iteration 694, loss = 0.02741301\n",
      "Iteration 695, loss = 0.02740232\n",
      "Iteration 696, loss = 0.02739049\n",
      "Iteration 697, loss = 0.02738292\n",
      "Iteration 698, loss = 0.02737339\n",
      "Iteration 699, loss = 0.02736228\n",
      "Iteration 700, loss = 0.02735276\n",
      "Iteration 701, loss = 0.02734300\n",
      "Iteration 702, loss = 0.02733339\n",
      "Iteration 703, loss = 0.02732335\n",
      "Iteration 704, loss = 0.02731398\n",
      "Iteration 705, loss = 0.02730318\n",
      "Iteration 706, loss = 0.02729243\n",
      "Iteration 707, loss = 0.02728341\n",
      "Iteration 708, loss = 0.02727393\n",
      "Iteration 709, loss = 0.02726433\n",
      "Iteration 710, loss = 0.02725406\n",
      "Iteration 711, loss = 0.02724376\n",
      "Iteration 712, loss = 0.02723275\n",
      "Iteration 713, loss = 0.02722444\n",
      "Iteration 714, loss = 0.02721539\n",
      "Iteration 715, loss = 0.02720457\n",
      "Iteration 716, loss = 0.02719401\n",
      "Iteration 717, loss = 0.02718452\n",
      "Iteration 718, loss = 0.02717467\n",
      "Iteration 719, loss = 0.02716595\n",
      "Iteration 720, loss = 0.02715587\n",
      "Iteration 721, loss = 0.02714516\n",
      "Iteration 722, loss = 0.02713561\n",
      "Iteration 723, loss = 0.02712659\n",
      "Iteration 724, loss = 0.02711670\n",
      "Iteration 725, loss = 0.02710623\n",
      "Iteration 726, loss = 0.02709578\n",
      "Iteration 727, loss = 0.02708556\n",
      "Iteration 728, loss = 0.02707663\n",
      "Iteration 729, loss = 0.02706755\n",
      "Iteration 730, loss = 0.02705719\n",
      "Iteration 731, loss = 0.02704723\n",
      "Iteration 732, loss = 0.02703873\n",
      "Iteration 733, loss = 0.02702895\n",
      "Iteration 734, loss = 0.02701816\n",
      "Iteration 735, loss = 0.02700737\n",
      "Iteration 736, loss = 0.02699727\n",
      "Iteration 737, loss = 0.02698880\n",
      "Iteration 738, loss = 0.02697840\n",
      "Iteration 739, loss = 0.02696845\n",
      "Iteration 740, loss = 0.02695969\n",
      "Iteration 741, loss = 0.02695060\n",
      "Iteration 742, loss = 0.02694108\n",
      "Iteration 743, loss = 0.02693059\n",
      "Iteration 744, loss = 0.02692074\n",
      "Iteration 745, loss = 0.02690566\n",
      "Iteration 746, loss = 0.02688832\n",
      "Iteration 747, loss = 0.02687053\n",
      "Iteration 748, loss = 0.02685351\n",
      "Iteration 749, loss = 0.02684149\n",
      "Iteration 750, loss = 0.02683320\n",
      "Iteration 751, loss = 0.02682456\n",
      "Iteration 752, loss = 0.02681529\n",
      "Iteration 753, loss = 0.02680595\n",
      "Iteration 754, loss = 0.02679514\n",
      "Iteration 755, loss = 0.02678342\n",
      "Iteration 756, loss = 0.02677160\n",
      "Iteration 757, loss = 0.02675933\n",
      "Iteration 758, loss = 0.02674670\n",
      "Iteration 759, loss = 0.02673564\n",
      "Iteration 760, loss = 0.02672391\n",
      "Iteration 761, loss = 0.02671626\n",
      "Iteration 762, loss = 0.02670634\n",
      "Iteration 763, loss = 0.02669597\n",
      "Iteration 764, loss = 0.02668372\n",
      "Iteration 765, loss = 0.02667192\n",
      "Iteration 766, loss = 0.02666441\n",
      "Iteration 767, loss = 0.02665428\n",
      "Iteration 768, loss = 0.02664250\n",
      "Iteration 769, loss = 0.02663186\n",
      "Iteration 770, loss = 0.02662154\n",
      "Iteration 771, loss = 0.02661011\n",
      "Iteration 772, loss = 0.02659963\n",
      "Iteration 773, loss = 0.02658959\n",
      "Iteration 774, loss = 0.02657980\n",
      "Iteration 775, loss = 0.02656994\n",
      "Iteration 776, loss = 0.02655937\n",
      "Iteration 777, loss = 0.02654693\n",
      "Iteration 778, loss = 0.02653807\n",
      "Iteration 779, loss = 0.02652926\n",
      "Iteration 780, loss = 0.02651707\n",
      "Iteration 781, loss = 0.02650530\n",
      "Iteration 782, loss = 0.02649601\n",
      "Iteration 783, loss = 0.02648527\n",
      "Iteration 784, loss = 0.02647601\n",
      "Iteration 785, loss = 0.02646632\n",
      "Iteration 786, loss = 0.02645546\n",
      "Iteration 787, loss = 0.02644480\n",
      "Iteration 788, loss = 0.02643513\n",
      "Iteration 789, loss = 0.02642612\n",
      "Iteration 790, loss = 0.02641524\n",
      "Iteration 791, loss = 0.02640580\n",
      "Iteration 792, loss = 0.02639547\n",
      "Iteration 793, loss = 0.02638538\n",
      "Iteration 794, loss = 0.02637407\n",
      "Iteration 795, loss = 0.02636395\n",
      "Iteration 796, loss = 0.02635508\n",
      "Iteration 797, loss = 0.02634486\n",
      "Iteration 798, loss = 0.02633273\n",
      "Iteration 799, loss = 0.02632282\n",
      "Iteration 800, loss = 0.02631263\n",
      "Iteration 801, loss = 0.02630251\n",
      "Iteration 802, loss = 0.02629274\n",
      "Iteration 803, loss = 0.02628256\n",
      "Iteration 804, loss = 0.02627312\n",
      "Iteration 805, loss = 0.02626172\n",
      "Iteration 806, loss = 0.02625205\n",
      "Iteration 807, loss = 0.02624257\n",
      "Iteration 808, loss = 0.02623261\n",
      "Iteration 809, loss = 0.02622339\n",
      "Iteration 810, loss = 0.02621199\n",
      "Iteration 811, loss = 0.02620335\n",
      "Iteration 812, loss = 0.02619447\n",
      "Iteration 813, loss = 0.02618346\n",
      "Iteration 814, loss = 0.02617201\n",
      "Iteration 815, loss = 0.02616207\n",
      "Iteration 816, loss = 0.02615236\n",
      "Iteration 817, loss = 0.02614360\n",
      "Iteration 818, loss = 0.02613308\n",
      "Iteration 819, loss = 0.02612377\n",
      "Iteration 820, loss = 0.02611296\n",
      "Iteration 821, loss = 0.02610423\n",
      "Iteration 822, loss = 0.02609427\n",
      "Iteration 823, loss = 0.02608462\n",
      "Iteration 824, loss = 0.02607340\n",
      "Iteration 825, loss = 0.02606205\n",
      "Iteration 826, loss = 0.02604960\n",
      "Iteration 827, loss = 0.02603937\n",
      "Iteration 828, loss = 0.02603179\n",
      "Iteration 829, loss = 0.02602162\n",
      "Iteration 830, loss = 0.02600981\n",
      "Iteration 831, loss = 0.02599821\n",
      "Iteration 832, loss = 0.02598547\n",
      "Iteration 833, loss = 0.02597592\n",
      "Iteration 834, loss = 0.02596713\n",
      "Iteration 835, loss = 0.02595605\n",
      "Iteration 836, loss = 0.02594461\n",
      "Iteration 837, loss = 0.02593370\n",
      "Iteration 838, loss = 0.02592306\n",
      "Iteration 839, loss = 0.02591194\n",
      "Iteration 840, loss = 0.02589936\n",
      "Iteration 841, loss = 0.02588867\n",
      "Iteration 842, loss = 0.02587956\n",
      "Iteration 843, loss = 0.02586846\n",
      "Iteration 844, loss = 0.02585654\n",
      "Iteration 845, loss = 0.02584406\n",
      "Iteration 846, loss = 0.02583300\n",
      "Iteration 847, loss = 0.02582353\n",
      "Iteration 848, loss = 0.02581358\n",
      "Iteration 849, loss = 0.02580321\n",
      "Iteration 850, loss = 0.02579167\n",
      "Iteration 851, loss = 0.02577874\n",
      "Iteration 852, loss = 0.02576664\n",
      "Iteration 853, loss = 0.02575752\n",
      "Iteration 854, loss = 0.02574659\n",
      "Iteration 855, loss = 0.02573467\n",
      "Iteration 856, loss = 0.02572347\n",
      "Iteration 857, loss = 0.02571186\n",
      "Iteration 858, loss = 0.02570116\n",
      "Iteration 859, loss = 0.02568840\n",
      "Iteration 860, loss = 0.02567884\n",
      "Iteration 861, loss = 0.02566751\n",
      "Iteration 862, loss = 0.02565646\n",
      "Iteration 863, loss = 0.02564569\n",
      "Iteration 864, loss = 0.02563316\n",
      "Iteration 865, loss = 0.02562138\n",
      "Iteration 866, loss = 0.02561136\n",
      "Iteration 867, loss = 0.02560076\n",
      "Iteration 868, loss = 0.02558886\n",
      "Iteration 869, loss = 0.02557822\n",
      "Iteration 870, loss = 0.02556674\n",
      "Iteration 871, loss = 0.02555405\n",
      "Iteration 872, loss = 0.02554371\n",
      "Iteration 873, loss = 0.02553319\n",
      "Iteration 874, loss = 0.02552229\n",
      "Iteration 875, loss = 0.02551001\n",
      "Iteration 876, loss = 0.02549950\n",
      "Iteration 877, loss = 0.02548940\n",
      "Iteration 878, loss = 0.02547785\n",
      "Iteration 879, loss = 0.02546836\n",
      "Iteration 880, loss = 0.02545745\n",
      "Iteration 881, loss = 0.02544540\n",
      "Iteration 882, loss = 0.02543300\n",
      "Iteration 883, loss = 0.02542278\n",
      "Iteration 884, loss = 0.02541089\n",
      "Iteration 885, loss = 0.02539872\n",
      "Iteration 886, loss = 0.02538972\n",
      "Iteration 887, loss = 0.02537993\n",
      "Iteration 888, loss = 0.02536841\n",
      "Iteration 889, loss = 0.02535666\n",
      "Iteration 890, loss = 0.02534576\n",
      "Iteration 891, loss = 0.02533400\n",
      "Iteration 892, loss = 0.02531990\n",
      "Iteration 893, loss = 0.02530559\n",
      "Iteration 894, loss = 0.02529059\n",
      "Iteration 895, loss = 0.02527392\n",
      "Iteration 896, loss = 0.02525675\n",
      "Iteration 897, loss = 0.02523826\n",
      "Iteration 898, loss = 0.02521897\n",
      "Iteration 899, loss = 0.02520191\n",
      "Iteration 900, loss = 0.02518459\n",
      "Iteration 901, loss = 0.02516245\n",
      "Iteration 902, loss = 0.02514640\n",
      "Iteration 903, loss = 0.02512949\n",
      "Iteration 904, loss = 0.02511228\n",
      "Iteration 905, loss = 0.02509442\n",
      "Iteration 906, loss = 0.02507594\n",
      "Iteration 907, loss = 0.02505752\n",
      "Iteration 908, loss = 0.02503871\n",
      "Iteration 909, loss = 0.02501825\n",
      "Iteration 910, loss = 0.02499897\n",
      "Iteration 911, loss = 0.02498819\n",
      "Iteration 912, loss = 0.02497548\n",
      "Iteration 913, loss = 0.02496231\n",
      "Iteration 914, loss = 0.02495060\n",
      "Iteration 915, loss = 0.02493835\n",
      "Iteration 916, loss = 0.02492592\n",
      "Iteration 917, loss = 0.02491663\n",
      "Iteration 918, loss = 0.02490633\n",
      "Iteration 919, loss = 0.02489579\n",
      "Iteration 920, loss = 0.02488369\n",
      "Iteration 921, loss = 0.02487182\n",
      "Iteration 922, loss = 0.02486153\n",
      "Iteration 923, loss = 0.02484986\n",
      "Iteration 924, loss = 0.02483800\n",
      "Iteration 925, loss = 0.02482655\n",
      "Iteration 926, loss = 0.02481373\n",
      "Iteration 927, loss = 0.02480407\n",
      "Iteration 928, loss = 0.02479562\n",
      "Iteration 929, loss = 0.02478435\n",
      "Iteration 930, loss = 0.02477234\n",
      "Iteration 931, loss = 0.02476187\n",
      "Iteration 932, loss = 0.02475298\n",
      "Iteration 933, loss = 0.02474292\n",
      "Iteration 934, loss = 0.02473008\n",
      "Iteration 935, loss = 0.02471934\n",
      "Iteration 936, loss = 0.02470981\n",
      "Iteration 937, loss = 0.02469887\n",
      "Iteration 938, loss = 0.02468625\n",
      "Iteration 939, loss = 0.02467636\n",
      "Iteration 940, loss = 0.02466637\n",
      "Iteration 941, loss = 0.02465445\n",
      "Iteration 942, loss = 0.02464513\n",
      "Iteration 943, loss = 0.02463487\n",
      "Iteration 944, loss = 0.02462348\n",
      "Iteration 945, loss = 0.02461359\n",
      "Iteration 946, loss = 0.02460325\n",
      "Iteration 947, loss = 0.02459252\n",
      "Iteration 948, loss = 0.02458156\n",
      "Iteration 949, loss = 0.02457119\n",
      "Iteration 950, loss = 0.02456052\n",
      "Iteration 951, loss = 0.02455082\n",
      "Iteration 952, loss = 0.02454097\n",
      "Iteration 953, loss = 0.02452997\n",
      "Iteration 954, loss = 0.02451963\n",
      "Iteration 955, loss = 0.02450836\n",
      "Iteration 956, loss = 0.02449695\n",
      "Iteration 957, loss = 0.02448785\n",
      "Iteration 958, loss = 0.02447702\n",
      "Iteration 959, loss = 0.02446787\n",
      "Iteration 960, loss = 0.02445603\n",
      "Iteration 961, loss = 0.02444684\n",
      "Iteration 962, loss = 0.02443621\n",
      "Iteration 963, loss = 0.02442649\n",
      "Iteration 964, loss = 0.02441499\n",
      "Iteration 965, loss = 0.02440444\n",
      "Iteration 966, loss = 0.02439412\n",
      "Iteration 967, loss = 0.02438409\n",
      "Iteration 968, loss = 0.02437392\n",
      "Iteration 969, loss = 0.02436238\n",
      "Iteration 970, loss = 0.02435268\n",
      "Iteration 971, loss = 0.02434332\n",
      "Iteration 972, loss = 0.02433339\n",
      "Iteration 973, loss = 0.02432074\n",
      "Iteration 974, loss = 0.02431139\n",
      "Iteration 975, loss = 0.02430127\n",
      "Iteration 976, loss = 0.02429105\n",
      "Iteration 977, loss = 0.02428003\n",
      "Iteration 978, loss = 0.02426947\n",
      "Iteration 979, loss = 0.02425976\n",
      "Iteration 980, loss = 0.02424907\n",
      "Iteration 981, loss = 0.02423824\n",
      "Iteration 982, loss = 0.02422845\n",
      "Iteration 983, loss = 0.02421830\n",
      "Iteration 984, loss = 0.02420800\n",
      "Iteration 985, loss = 0.02419803\n",
      "Iteration 986, loss = 0.02418835\n",
      "Iteration 987, loss = 0.02417825\n",
      "Iteration 988, loss = 0.02416698\n",
      "Iteration 989, loss = 0.02415534\n",
      "Iteration 990, loss = 0.02414565\n",
      "Iteration 991, loss = 0.02413674\n",
      "Iteration 992, loss = 0.02412627\n",
      "Iteration 993, loss = 0.02411458\n",
      "Iteration 994, loss = 0.02410626\n",
      "Iteration 995, loss = 0.02409674\n",
      "Iteration 996, loss = 0.02408681\n",
      "Iteration 997, loss = 0.02407531\n",
      "Iteration 998, loss = 0.02406469\n",
      "Iteration 999, loss = 0.02405562\n",
      "Iteration 1000, loss = 0.02404563\n",
      "Iteration 1001, loss = 0.02403593\n",
      "Iteration 1002, loss = 0.02402461\n",
      "Iteration 1003, loss = 0.02401407\n",
      "Iteration 1004, loss = 0.02400300\n",
      "Iteration 1005, loss = 0.02399304\n",
      "Iteration 1006, loss = 0.02398372\n",
      "Iteration 1007, loss = 0.02397409\n",
      "Iteration 1008, loss = 0.02396359\n",
      "Iteration 1009, loss = 0.02395412\n",
      "Iteration 1010, loss = 0.02394330\n",
      "Iteration 1011, loss = 0.02393348\n",
      "Iteration 1012, loss = 0.02392245\n",
      "Iteration 1013, loss = 0.02391352\n",
      "Iteration 1014, loss = 0.02390450\n",
      "Iteration 1015, loss = 0.02389295\n",
      "Iteration 1016, loss = 0.02388108\n",
      "Iteration 1017, loss = 0.02387251\n",
      "Iteration 1018, loss = 0.02386280\n",
      "Iteration 1019, loss = 0.02385161\n",
      "Iteration 1020, loss = 0.02384138\n",
      "Iteration 1021, loss = 0.02383205\n",
      "Iteration 1022, loss = 0.02382274\n",
      "Iteration 1023, loss = 0.02381178\n",
      "Iteration 1024, loss = 0.02380084\n",
      "Iteration 1025, loss = 0.02379151\n",
      "Iteration 1026, loss = 0.02378269\n",
      "Iteration 1027, loss = 0.02377166\n",
      "Iteration 1028, loss = 0.02376363\n",
      "Iteration 1029, loss = 0.02375406\n",
      "Iteration 1030, loss = 0.02374353\n",
      "Iteration 1031, loss = 0.02373263\n",
      "Iteration 1032, loss = 0.02372204\n",
      "Iteration 1033, loss = 0.02371320\n",
      "Iteration 1034, loss = 0.02370340\n",
      "Iteration 1035, loss = 0.02369262\n",
      "Iteration 1036, loss = 0.02368152\n",
      "Iteration 1037, loss = 0.02367238\n",
      "Iteration 1038, loss = 0.02366386\n",
      "Iteration 1039, loss = 0.02365420\n",
      "Iteration 1040, loss = 0.02364323\n",
      "Iteration 1041, loss = 0.02363335\n",
      "Iteration 1042, loss = 0.02362433\n",
      "Iteration 1043, loss = 0.02361458\n",
      "Iteration 1044, loss = 0.02360558\n",
      "Iteration 1045, loss = 0.02359535\n",
      "Iteration 1046, loss = 0.02358344\n",
      "Iteration 1047, loss = 0.02357404\n",
      "Iteration 1048, loss = 0.02356564\n",
      "Iteration 1049, loss = 0.02355480\n",
      "Iteration 1050, loss = 0.02354416\n",
      "Iteration 1051, loss = 0.02353384\n",
      "Iteration 1052, loss = 0.02352670\n",
      "Iteration 1053, loss = 0.02351534\n",
      "Iteration 1054, loss = 0.02350423\n",
      "Iteration 1055, loss = 0.02349712\n",
      "Iteration 1056, loss = 0.02348797\n",
      "Iteration 1057, loss = 0.02347789\n",
      "Iteration 1058, loss = 0.02346610\n",
      "Iteration 1059, loss = 0.02345587\n",
      "Iteration 1060, loss = 0.02344776\n",
      "Iteration 1061, loss = 0.02343764\n",
      "Iteration 1062, loss = 0.02342680\n",
      "Iteration 1063, loss = 0.02341760\n",
      "Iteration 1064, loss = 0.02340824\n",
      "Iteration 1065, loss = 0.02339832\n",
      "Iteration 1066, loss = 0.02338739\n",
      "Iteration 1067, loss = 0.02338046\n",
      "Iteration 1068, loss = 0.02336998\n",
      "Iteration 1069, loss = 0.02335991\n",
      "Iteration 1070, loss = 0.02335087\n",
      "Iteration 1071, loss = 0.02334028\n",
      "Iteration 1072, loss = 0.02333164\n",
      "Iteration 1073, loss = 0.02332263\n",
      "Iteration 1074, loss = 0.02331324\n",
      "Iteration 1075, loss = 0.02330238\n",
      "Iteration 1076, loss = 0.02329200\n",
      "Iteration 1077, loss = 0.02328257\n",
      "Iteration 1078, loss = 0.02327431\n",
      "Iteration 1079, loss = 0.02326441\n",
      "Iteration 1080, loss = 0.02325246\n",
      "Iteration 1081, loss = 0.02324246\n",
      "Iteration 1082, loss = 0.02323439\n",
      "Iteration 1083, loss = 0.02322607\n",
      "Iteration 1084, loss = 0.02321671\n",
      "Iteration 1085, loss = 0.02320553\n",
      "Iteration 1086, loss = 0.02319625\n",
      "Iteration 1087, loss = 0.02318690\n",
      "Iteration 1088, loss = 0.02317778\n",
      "Iteration 1089, loss = 0.02316700\n",
      "Iteration 1090, loss = 0.02315798\n",
      "Iteration 1091, loss = 0.02314692\n",
      "Iteration 1092, loss = 0.02313758\n",
      "Iteration 1093, loss = 0.02312860\n",
      "Iteration 1094, loss = 0.02311994\n",
      "Iteration 1095, loss = 0.02310829\n",
      "Iteration 1096, loss = 0.02309961\n",
      "Iteration 1097, loss = 0.02309114\n",
      "Iteration 1098, loss = 0.02308282\n",
      "Iteration 1099, loss = 0.02307373\n",
      "Iteration 1100, loss = 0.02306444\n",
      "Iteration 1101, loss = 0.02305251\n",
      "Iteration 1102, loss = 0.02304129\n",
      "Iteration 1103, loss = 0.02302852\n",
      "Iteration 1104, loss = 0.02301504\n",
      "Iteration 1105, loss = 0.02300163\n",
      "Iteration 1106, loss = 0.02298490\n",
      "Iteration 1107, loss = 0.02296880\n",
      "Iteration 1108, loss = 0.02295377\n",
      "Iteration 1109, loss = 0.02293920\n",
      "Iteration 1110, loss = 0.02292343\n",
      "Iteration 1111, loss = 0.02290541\n",
      "Iteration 1112, loss = 0.02288708\n",
      "Iteration 1113, loss = 0.02287001\n",
      "Iteration 1114, loss = 0.02285407\n",
      "Iteration 1115, loss = 0.02283692\n",
      "Iteration 1116, loss = 0.02282991\n",
      "Iteration 1117, loss = 0.02282504\n",
      "Iteration 1118, loss = 0.02281702\n",
      "Iteration 1119, loss = 0.02281170\n",
      "Iteration 1120, loss = 0.02280513\n",
      "Iteration 1121, loss = 0.02279619\n",
      "Iteration 1122, loss = 0.02278420\n",
      "Iteration 1123, loss = 0.02277264\n",
      "Iteration 1124, loss = 0.02276051\n",
      "Iteration 1125, loss = 0.02274483\n",
      "Iteration 1126, loss = 0.02273191\n",
      "Iteration 1127, loss = 0.02272015\n",
      "Iteration 1128, loss = 0.02270736\n",
      "Iteration 1129, loss = 0.02270196\n",
      "Iteration 1130, loss = 0.02269269\n",
      "Iteration 1131, loss = 0.02268140\n",
      "Iteration 1132, loss = 0.02267108\n",
      "Iteration 1133, loss = 0.02266139\n",
      "Iteration 1134, loss = 0.02264931\n",
      "Iteration 1135, loss = 0.02263585\n",
      "Iteration 1136, loss = 0.02262751\n",
      "Iteration 1137, loss = 0.02262125\n",
      "Iteration 1138, loss = 0.02261108\n",
      "Iteration 1139, loss = 0.02260052\n",
      "Iteration 1140, loss = 0.02259042\n",
      "Iteration 1141, loss = 0.02257952\n",
      "Iteration 1142, loss = 0.02256603\n",
      "Iteration 1143, loss = 0.02255731\n",
      "Iteration 1144, loss = 0.02254769\n",
      "Iteration 1145, loss = 0.02253817\n",
      "Iteration 1146, loss = 0.02252888\n",
      "Iteration 1147, loss = 0.02251844\n",
      "Iteration 1148, loss = 0.02250789\n",
      "Iteration 1149, loss = 0.02249943\n",
      "Iteration 1150, loss = 0.02248971\n",
      "Iteration 1151, loss = 0.02247704\n",
      "Iteration 1152, loss = 0.02246862\n",
      "Iteration 1153, loss = 0.02246128\n",
      "Iteration 1154, loss = 0.02245028\n",
      "Iteration 1155, loss = 0.02243969\n",
      "Iteration 1156, loss = 0.02242935\n",
      "Iteration 1157, loss = 0.02242029\n",
      "Iteration 1158, loss = 0.02241091\n",
      "Iteration 1159, loss = 0.02240227\n",
      "Iteration 1160, loss = 0.02239220\n",
      "Iteration 1161, loss = 0.02238057\n",
      "Iteration 1162, loss = 0.02237318\n",
      "Iteration 1163, loss = 0.02236527\n",
      "Iteration 1164, loss = 0.02235556\n",
      "Iteration 1165, loss = 0.02234492\n",
      "Iteration 1166, loss = 0.02233744\n",
      "Iteration 1167, loss = 0.02232785\n",
      "Iteration 1168, loss = 0.02231985\n",
      "Iteration 1169, loss = 0.02231191\n",
      "Iteration 1170, loss = 0.02230420\n",
      "Iteration 1171, loss = 0.02229429\n",
      "Iteration 1172, loss = 0.02228355\n",
      "Iteration 1173, loss = 0.02227140\n",
      "Iteration 1174, loss = 0.02226324\n",
      "Iteration 1175, loss = 0.02225620\n",
      "Iteration 1176, loss = 0.02224210\n",
      "Iteration 1177, loss = 0.02223577\n",
      "Iteration 1178, loss = 0.02222817\n",
      "Iteration 1179, loss = 0.02221890\n",
      "Iteration 1180, loss = 0.02220926\n",
      "Iteration 1181, loss = 0.02219761\n",
      "Iteration 1182, loss = 0.02218866\n",
      "Iteration 1183, loss = 0.02217997\n",
      "Iteration 1184, loss = 0.02217068\n",
      "Iteration 1185, loss = 0.02216220\n",
      "Iteration 1186, loss = 0.02215250\n",
      "Iteration 1187, loss = 0.02214431\n",
      "Iteration 1188, loss = 0.02213463\n",
      "Iteration 1189, loss = 0.02212492\n",
      "Iteration 1190, loss = 0.02211464\n",
      "Iteration 1191, loss = 0.02210637\n",
      "Iteration 1192, loss = 0.02209814\n",
      "Iteration 1193, loss = 0.02208684\n",
      "Iteration 1194, loss = 0.02207756\n",
      "Iteration 1195, loss = 0.02206816\n",
      "Iteration 1196, loss = 0.02205937\n",
      "Iteration 1197, loss = 0.02205019\n",
      "Iteration 1198, loss = 0.02204199\n",
      "Iteration 1199, loss = 0.02203217\n",
      "Iteration 1200, loss = 0.02202481\n",
      "Iteration 1201, loss = 0.02201375\n",
      "Iteration 1202, loss = 0.02200414\n",
      "Iteration 1203, loss = 0.02199490\n",
      "Iteration 1204, loss = 0.02198672\n",
      "Iteration 1205, loss = 0.02197769\n",
      "Iteration 1206, loss = 0.02196889\n",
      "Iteration 1207, loss = 0.02195902\n",
      "Iteration 1208, loss = 0.02194954\n",
      "Iteration 1209, loss = 0.02193940\n",
      "Iteration 1210, loss = 0.02193013\n",
      "Iteration 1211, loss = 0.02192316\n",
      "Iteration 1212, loss = 0.02191296\n",
      "Iteration 1213, loss = 0.02190391\n",
      "Iteration 1214, loss = 0.02189442\n",
      "Iteration 1215, loss = 0.02188484\n",
      "Iteration 1216, loss = 0.02187876\n",
      "Iteration 1217, loss = 0.02186702\n",
      "Iteration 1218, loss = 0.02185791\n",
      "Iteration 1219, loss = 0.02184871\n",
      "Iteration 1220, loss = 0.02184159\n",
      "Iteration 1221, loss = 0.02183133\n",
      "Iteration 1222, loss = 0.02182274\n",
      "Iteration 1223, loss = 0.02181296\n",
      "Iteration 1224, loss = 0.02180317\n",
      "Iteration 1225, loss = 0.02179746\n",
      "Iteration 1226, loss = 0.02178923\n",
      "Iteration 1227, loss = 0.02177880\n",
      "Iteration 1228, loss = 0.02177026\n",
      "Iteration 1229, loss = 0.02176044\n",
      "Iteration 1230, loss = 0.02175327\n",
      "Iteration 1231, loss = 0.02174315\n",
      "Iteration 1232, loss = 0.02173335\n",
      "Iteration 1233, loss = 0.02172399\n",
      "Iteration 1234, loss = 0.02171701\n",
      "Iteration 1235, loss = 0.02170878\n",
      "Iteration 1236, loss = 0.02169878\n",
      "Iteration 1237, loss = 0.02168956\n",
      "Iteration 1238, loss = 0.02167926\n",
      "Iteration 1239, loss = 0.02167004\n",
      "Iteration 1240, loss = 0.02166208\n",
      "Iteration 1241, loss = 0.02165344\n",
      "Iteration 1242, loss = 0.02164332\n",
      "Iteration 1243, loss = 0.02163543\n",
      "Iteration 1244, loss = 0.02162647\n",
      "Iteration 1245, loss = 0.02161761\n",
      "Iteration 1246, loss = 0.02160918\n",
      "Iteration 1247, loss = 0.02160019\n",
      "Iteration 1248, loss = 0.02158991\n",
      "Iteration 1249, loss = 0.02158226\n",
      "Iteration 1250, loss = 0.02157239\n",
      "Iteration 1251, loss = 0.02156342\n",
      "Iteration 1252, loss = 0.02155558\n",
      "Iteration 1253, loss = 0.02154628\n",
      "Iteration 1254, loss = 0.02153677\n",
      "Iteration 1255, loss = 0.02152808\n",
      "Iteration 1256, loss = 0.02151970\n",
      "Iteration 1257, loss = 0.02150972\n",
      "Iteration 1258, loss = 0.02150293\n",
      "Iteration 1259, loss = 0.02149459\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Two more MLP models \n",
    "\n",
    "#For evaluating wsim and wnn weights dependence on input values X_train\n",
    "\n",
    "\n",
    "mlpRwsim = MLPRegressor(hidden_layer_sizes=(hidden_nodes,hidden_nodes), max_iter=10000, alpha=1e-4,\n",
    "                    solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
    "                    learning_rate_init=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "mlpRwnn = MLPRegressor(hidden_layer_sizes=(hidden_nodes,hidden_nodes), max_iter=10000, alpha=1e-4,\n",
    "                    solver='adam', verbose=10, tol=1e-5, random_state=1,\n",
    "                    learning_rate_init=1e-4)\n",
    "\n",
    "mlpRwsim.fit(X_train, wsim)\n",
    "wsim_predict=mlpRwsim.predict(X_train)\n",
    "wsim_test_predict=mlpRwsim.predict(X_test)\n",
    "mlpRwnn.fit(X_train, wnn)\n",
    "wnn_predict=mlpRwnn.predict(X_train)\n",
    "wnn_test_predict=mlpRwnn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AWNN Prediction on  X_train values\n",
    "\n",
    "yf_predict=y_predict*wnn_predict+y_sim*wsim_predict\n",
    "plt.figure(2)\n",
    "plt.plot(t, scaler2.inverse_transform(y_train.reshape(-1,1)), \n",
    "         'r--', t, scaler2.inverse_transform(y_predict.reshape(-1,1)), 'b.', \n",
    "         t, scaler2.inverse_transform(yf_predict.reshape(-1,1)), 'y.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011097139325693032\n",
      "0.0011571321596215648\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AWNN Prediction on  X_test values\n",
    "\n",
    "ytest_predict=mlp.predict(X_test)\n",
    "yftest_predict=ytest_predict*wnn_test_predict+ytest_sim*wsim_test_predict\n",
    "t = np.arange(0, len(y_test),1)\n",
    "plt.figure(3)\n",
    "plt.plot(t, scaler2.inverse_transform(y_test.reshape(-1,1)), 'r--', \n",
    "         t, scaler2.inverse_transform(ytest_predict.reshape(-1,1)), 'b.', \n",
    "         t, scaler2.inverse_transform(yftest_predict.reshape(-1,1)), 'y.')\n",
    "\n",
    "print (mean_squared_error(y_test.reshape(-1,1), ytest_predict.reshape(-1,1) )  )\n",
    "print (mean_squared_error(y_test.reshape(-1,1), yftest_predict.reshape(-1,1) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"foo.csv\", t, delimiter=\",\")\n",
    "np.savetxt(\"foo1.csv\", y_test, delimiter=\",\")\n",
    "np.savetxt(\"foo2.csv\", ytest_predict, delimiter=\",\")\n",
    "np.savetxt(\"foo3.csv\", yftest_predict, delimiter=\",\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
